#!/usr/bin/env python3
"""
Off-Policy Monte Carlo Visual Demonstration

This script implements a Gridworld environment and demonstrates off-policy
Monte Carlo evaluation using importance sampling. It shows:

- A fixed target policy (e.g., always move toward goal)
- A different behavior policy (e.g., uniform random or biased)
- Real-time visualization of:
  - Episodes generated by behavior policy (trajectory overlay)
  - Importance weights per episode
  - Ordinary and Weighted IS estimates of V(start) for the target policy
  - Histogram of importance weights

Usage:
    python slides/examples/mc_offpolicy_visual.py

Requires:
    numpy, matplotlib

Notes for students:
- The behavior policy generates episodes (what we observe)
- The target policy is what we want to evaluate (what we care about)
- Importance sampling corrects for the mismatch between policies
- Watch how IS variance depends on policy overlap
"""

import random
from collections import defaultdict, deque
from typing import Tuple, List, Dict, Callable
import numpy as np
import matplotlib.pyplot as plt

# -------------------------
# Gridworld environment (same as on-policy version)
# -------------------------


class GridWorld:
    def __init__(
        self,
        n_rows: int = 5,
        n_cols: int = 5,
        start: Tuple[int, int] = None,
        goal: Tuple[int, int] = None,
        hole: Tuple[int, int] = None,
    ):
        self.n_rows = n_rows
        self.n_cols = n_cols
        self.start = start if start is not None else (n_rows - 1, 0)
        self.goal = goal if goal is not None else (0, n_cols - 1)
        self.hole = hole if hole is not None else (1, 1)
        self.state = self.start
        self.actions = [0, 1, 2, 3]  # UP, RIGHT, DOWN, LEFT

    def reset(self):
        self.state = self.start
        return self.state

    def step(self, action: int):
        r, c = self.state
        if action == 0:  # UP
            r2, c2 = max(0, r - 1), c
        elif action == 1:  # RIGHT
            r2, c2 = r, min(self.n_cols - 1, c + 1)
        elif action == 2:  # DOWN
            r2, c2 = min(self.n_rows - 1, r + 1), c
        elif action == 3:  # LEFT
            r2, c2 = r, max(0, c - 1)
        else:
            raise ValueError("Invalid action")

        self.state = (r2, c2)
        if self.state == self.goal:
            return self.state, 1.0, True
        if self.state == self.hole:
            return self.state, -1.0, True
        return self.state, 0.0, False

    def state_space(self):
        return [(r, c) for r in range(self.n_rows) for c in range(self.n_cols)]

    def is_terminal(self, s):
        return s == self.goal or s == self.hole


# -------------------------
# Policy classes
# -------------------------


class TargetPolicy:
    """Deterministic policy that moves toward the goal."""

    def __init__(self, env: GridWorld):
        self.env = env
        self.goal = env.goal

    def __call__(self, s: Tuple[int, int]) -> int:
        if self.env.is_terminal(s):
            return 0  # arbitrary for terminal states

        r, c = s
        gr, gc = self.goal

        # Simple heuristic: move toward goal (prefer horizontal then vertical)
        if c < gc:
            return 1  # RIGHT
        elif c > gc:
            return 3  # LEFT
        elif r > gr:
            return 0  # UP
        elif r < gr:
            return 2  # DOWN
        else:
            return 1  # arbitrary if at goal

    def prob(self, s: Tuple[int, int], a: int) -> float:
        return 1.0 if self(s) == a else 0.0


class UniformBehaviorPolicy:
    """Uniform random behavior policy."""

    def __init__(self, n_actions: int = 4):
        self.n_actions = n_actions

    def __call__(self, s: Tuple[int, int]) -> int:
        return random.randrange(self.n_actions)

    def prob(self, s: Tuple[int, int], a: int) -> float:
        return 1.0 / self.n_actions


class BiasedBehaviorPolicy:
    """Behavior policy biased toward certain actions."""

    def __init__(self, action_probs: List[float] = None):
        if action_probs is None:
            # Default: biased toward LEFT and DOWN (away from typical goal)
            action_probs = [0.2, 0.2, 0.4, 0.2]  # UP, RIGHT, DOWN, LEFT
        assert abs(sum(action_probs) - 1.0) < 1e-6, "Probabilities must sum to 1"
        self.action_probs = action_probs
        self.n_actions = len(action_probs)

    def __call__(self, s: Tuple[int, int]) -> int:
        return np.random.choice(self.n_actions, p=self.action_probs)

    def prob(self, s: Tuple[int, int], a: int) -> float:
        return self.action_probs[a]


class EpsilonSoftTargetPolicy:
    """Epsilon-soft version of target policy for better IS overlap."""

    def __init__(
        self, base_policy: TargetPolicy, epsilon: float = 0.1, n_actions: int = 4
    ):
        self.base_policy = base_policy
        self.epsilon = epsilon
        self.n_actions = n_actions

    def __call__(self, s: Tuple[int, int]) -> int:
        if random.random() < self.epsilon:
            return random.randrange(self.n_actions)
        return self.base_policy(s)

    def prob(self, s: Tuple[int, int], a: int) -> float:
        greedy_a = self.base_policy(s)
        if a == greedy_a:
            return 1.0 - self.epsilon + self.epsilon / self.n_actions
        else:
            return self.epsilon / self.n_actions


# -------------------------
# Episode generation and IS evaluation
# -------------------------


def generate_episode(
    env: GridWorld, policy_fn: Callable
) -> List[Tuple[Tuple[int, int], int, float]]:
    """Generate episode following policy_fn. Returns list of (s, a, r)."""
    episode = []
    s = env.reset()
    while True:
        a = policy_fn(s)
        s2, r, done = env.step(a)
        episode.append((s, a, r))
        s = s2  # Update current state for next iteration
        if done:
            break
    return episode


def compute_importance_weight(episode: List, target_policy, behavior_policy) -> float:
    """Compute trajectory-level importance weight W = prod_t pi(a_t|s_t) / b(a_t|s_t)."""
    W = 1.0
    for s, a, _ in episode:
        pi = target_policy.prob(s, a)
        b = behavior_policy.prob(s, a)
        if b == 0.0:
            return 0.0
        W *= pi / b
    return W


def compute_return(episode: List, gamma: float = 1.0) -> float:
    """Compute discounted return for episode."""
    G = 0.0
    for t, (_, _, r) in enumerate(episode):
        G += (gamma**t) * r
    return G


# -------------------------
# Visualization functions
# -------------------------


def plot_episode_trajectory(ax, env: GridWorld, episode: List, alpha: float = 0.3):
    """Overlay episode trajectory on grid."""
    if len(episode) == 0:
        return

    # Extract state sequence
    states = [s for s, _, _ in episode]
    if states:
        rows = [s[0] for s in states]
        cols = [s[1] for s in states]

        # Plot trajectory as connected line
        ax.plot(cols, rows, "r-", alpha=alpha, linewidth=2, marker="o", markersize=4)

        # Mark start and end
        ax.plot(cols[0], rows[0], "go", markersize=8, label="Episode start")
        if len(cols) > 1:
            ax.plot(cols[-1], rows[-1], "ro", markersize=8, label="Episode end")


def plot_target_policy_arrows(ax, env: GridWorld, target_policy):
    """Draw arrows showing target policy."""
    X, Y, U, V = [], [], [], []
    for r in range(env.n_rows):
        for c in range(env.n_cols):
            s = (r, c)
            if env.is_terminal(s):
                continue

            a = target_policy(s)
            # Convert action to arrow direction
            dx, dy = 0.0, 0.0
            if a == 0:  # UP
                dy = -0.3
            elif a == 1:  # RIGHT
                dx = 0.3
            elif a == 2:  # DOWN
                dy = 0.3
            elif a == 3:  # LEFT
                dx = -0.3

            X.append(c)
            Y.append(r)
            U.append(dx)
            V.append(dy)

    if X:
        ax.quiver(
            X,
            Y,
            U,
            V,
            angles="xy",
            scale_units="xy",
            scale=1.0,
            color="blue",
            alpha=0.7,
            width=0.003,
        )


def setup_grid_plot(ax, env: GridWorld):
    """Setup grid visualization with labels."""
    # Draw grid
    ax.set_xlim(-0.5, env.n_cols - 0.5)
    ax.set_ylim(-0.5, env.n_rows - 0.5)
    ax.set_aspect("equal")

    # Grid lines
    for i in range(env.n_rows + 1):
        ax.axhline(i - 0.5, color="gray", linewidth=0.5)
    for i in range(env.n_cols + 1):
        ax.axvline(i - 0.5, color="gray", linewidth=0.5)

    # Mark special states
    sr, sc = env.start
    gr, gc = env.goal
    hr, hc = env.hole

    ax.text(
        sc,
        sr,
        "S",
        ha="center",
        va="center",
        fontsize=14,
        fontweight="bold",
        color="green",
    )
    ax.text(
        gc,
        gr,
        "G",
        ha="center",
        va="center",
        fontsize=14,
        fontweight="bold",
        color="blue",
    )
    ax.text(
        hc,
        hr,
        "H",
        ha="center",
        va="center",
        fontsize=14,
        fontweight="bold",
        color="red",
    )

    ax.set_xticks(range(env.n_cols))
    ax.set_yticks(range(env.n_rows))
    ax.invert_yaxis()  # Match array indexing


# -------------------------
# Main off-policy visualization
# -------------------------


def run_offpolicy_demo(
    n_rows: int = 5,
    n_cols: int = 5,
    start: Tuple[int, int] = None,
    goal: Tuple[int, int] = None,
    hole: Tuple[int, int] = None,
    num_episodes: int = 1000,
    gamma: float = 1.0,
    update_interval: int = 10,
    behavior_type: str = "uniform",  # "uniform" or "biased"
    target_epsilon: float = 0.0,  # 0 for deterministic, >0 for epsilon-soft
):
    """
    Run off-policy Monte Carlo evaluation with real-time visualization.
    """
    env = GridWorld(n_rows=n_rows, n_cols=n_cols, start=start, goal=goal, hole=hole)

    # Create policies
    base_target = TargetPolicy(env)
    if target_epsilon > 0:
        target_policy = EpsilonSoftTargetPolicy(base_target, target_epsilon)
    else:
        target_policy = base_target

    if behavior_type == "uniform":
        behavior_policy = UniformBehaviorPolicy(n_actions=4)
    else:  # biased
        behavior_policy = BiasedBehaviorPolicy(
            [0.15, 0.15, 0.4, 0.3]
        )  # biased away from goal

    # Initialize tracking variables
    ordinary_estimates = []
    weighted_estimates = []
    weights_history = []
    episodes_history = []

    cum_ordinary = 0.0
    cum_weighted_num = 0.0
    cum_weight = 0.0

    # Setup plots
    plt.ion()
    fig, axes = plt.subplots(2, 3, figsize=(15, 10))
    fig.suptitle("Off-Policy Monte Carlo: Behavior vs Target Policy", fontsize=14)

    # Plot layout:
    # Top: [Grid+Trajectory, IS Estimates, Weight Distribution]
    # Bottom: [Target Policy, Behavior Info, Weight Timeline]

    ax_grid = axes[0, 0]
    ax_estimates = axes[0, 1]
    ax_weight_dist = axes[0, 2]
    ax_target = axes[1, 0]
    ax_behavior = axes[1, 1]
    ax_weight_time = axes[1, 2]

    # Setup grid plots
    setup_grid_plot(ax_grid, env)
    ax_grid.set_title("Latest Episode (Behavior Policy)")

    setup_grid_plot(ax_target, env)
    plot_target_policy_arrows(ax_target, env, target_policy)
    ax_target.set_title("Target Policy (Blue Arrows)")

    # Behavior policy info
    ax_behavior.text(
        0.1,
        0.8,
        f"Behavior: {behavior_type}",
        transform=ax_behavior.transAxes,
        fontsize=12,
    )
    if behavior_type == "biased":
        probs = behavior_policy.action_probs
        ax_behavior.text(
            0.1,
            0.6,
            f"Action probs: U={probs[0]:.2f}, R={probs[1]:.2f}",
            transform=ax_behavior.transAxes,
            fontsize=10,
        )
        ax_behavior.text(
            0.1,
            0.5,
            f"D={probs[2]:.2f}, L={probs[3]:.2f}",
            transform=ax_behavior.transAxes,
            fontsize=10,
        )
    else:
        ax_behavior.text(
            0.1, 0.6, "All actions: 0.25", transform=ax_behavior.transAxes, fontsize=10
        )

    ax_behavior.text(
        0.1,
        0.3,
        f"Target: {'ε-soft' if target_epsilon > 0 else 'deterministic'}",
        transform=ax_behavior.transAxes,
        fontsize=12,
    )
    if target_epsilon > 0:
        ax_behavior.text(
            0.1,
            0.2,
            f"ε = {target_epsilon}",
            transform=ax_behavior.transAxes,
            fontsize=10,
        )

    ax_behavior.set_xlim(0, 1)
    ax_behavior.set_ylim(0, 1)
    ax_behavior.axis("off")

    # Main simulation loop
    try:
        for ep in range(1, num_episodes + 1):
            # Generate episode using behavior policy
            episode = generate_episode(env, behavior_policy)
            episodes_history.append(episode)

            # Compute return and importance weight
            G = compute_return(episode, gamma)
            W = compute_importance_weight(episode, target_policy, behavior_policy)
            weights_history.append(W)

            # Update IS estimates
            cum_ordinary += W * G
            ordinary_estimates.append(cum_ordinary / ep)

            cum_weight += W
            cum_weighted_num += W * G
            if cum_weight == 0:
                weighted_estimates.append(0.0)
            else:
                weighted_estimates.append(cum_weighted_num / cum_weight)

            # Update visualization
            if ep % update_interval == 0 or ep == 1 or ep == num_episodes:
                # Clear and update grid with latest episode
                ax_grid.clear()
                setup_grid_plot(ax_grid, env)
                plot_episode_trajectory(ax_grid, env, episode)
                ax_grid.set_title(f"Episode {ep}: W = {W:.3f}, G = {G:.2f}")

                # Update IS estimates plot
                ax_estimates.clear()
                episodes_range = range(1, len(ordinary_estimates) + 1)
                ax_estimates.plot(
                    episodes_range,
                    ordinary_estimates,
                    "b-",
                    alpha=0.7,
                    label="Ordinary IS",
                )
                ax_estimates.plot(
                    episodes_range,
                    weighted_estimates,
                    "r-",
                    alpha=0.9,
                    label="Weighted IS",
                )
                ax_estimates.axhline(0, color="k", linestyle="--", alpha=0.5)
                ax_estimates.set_title(f"V(start) Estimates")
                ax_estimates.set_xlabel("Episode")
                ax_estimates.set_ylabel("Estimated V(start)")
                ax_estimates.legend()
                ax_estimates.grid(True, alpha=0.3)

                # Weight distribution histogram
                ax_weight_dist.clear()
                if len(weights_history) > 1:
                    weights_arr = np.array(weights_history)
                    nonzero_weights = weights_arr[weights_arr > 0]
                    if len(nonzero_weights) > 0:
                        ax_weight_dist.hist(
                            nonzero_weights,
                            bins=min(20, len(nonzero_weights)),
                            alpha=0.7,
                            edgecolor="black",
                        )
                        ax_weight_dist.set_xlabel("Importance Weight")
                        ax_weight_dist.set_ylabel("Frequency")
                        ax_weight_dist.set_title(
                            f"Weight Distribution\n({len(nonzero_weights)}/{len(weights_history)} nonzero)"
                        )
                    else:
                        ax_weight_dist.text(
                            0.5,
                            0.5,
                            "All weights = 0",
                            ha="center",
                            va="center",
                            transform=ax_weight_dist.transAxes,
                        )
                        ax_weight_dist.set_title("Weight Distribution")

                # Weight timeline
                ax_weight_time.clear()
                ax_weight_time.plot(
                    range(1, len(weights_history) + 1), weights_history, "g-", alpha=0.6
                )
                ax_weight_time.set_xlabel("Episode")
                ax_weight_time.set_ylabel("Weight")
                ax_weight_time.set_title("Importance Weights Over Time")
                ax_weight_time.grid(True, alpha=0.3)

                plt.tight_layout()
                plt.pause(0.01)

        # Final statistics
        final_ordinary = ordinary_estimates[-1] if ordinary_estimates else 0.0
        final_weighted = weighted_estimates[-1] if weighted_estimates else 0.0
        nonzero_frac = (
            np.count_nonzero(weights_history) / len(weights_history)
            if weights_history
            else 0.0
        )

        print(f"\nFinal Results after {num_episodes} episodes:")
        print(f"Ordinary IS estimate: {final_ordinary:.4f}")
        print(f"Weighted IS estimate: {final_weighted:.4f}")
        print(f"Fraction of nonzero weights: {nonzero_frac:.4f}")
        print(f"Max weight: {max(weights_history) if weights_history else 0:.4f}")

        plt.ioff()
        plt.show()

    except KeyboardInterrupt:
        print("\nInterrupted by user.")
        plt.ioff()
        plt.show()


# -------------------------
# Entrypoint with examples
# -------------------------

if __name__ == "__main__":
    run_offpolicy_demo(
        num_episodes=1000,
        behavior_type="uniform",
        target_epsilon=0.1,
        update_interval=15,
        gamma=0.9,
    )
