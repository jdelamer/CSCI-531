#!/usr/bin/env python3
"""
mc_compare_on_off.py

Compare On-Policy First-Visit Monte Carlo Control (epsilon-greedy) with
Off-Policy Monte Carlo Evaluation using Importance Sampling in a small Gridworld.

What the script does:
- Constructs a Gridworld with Start (S), Goal (G, +1), and Hole (H, -1).
- Computes the ground-truth optimal state-value function using Value Iteration.
- Runs On-Policy First-Visit MC Control (epsilon-greedy) to learn Q and a policy;
  tracks V(start) = max_a Q(start,a) over episodes and displays learned V heatmap.
- Runs Off-Policy MC Evaluation to estimate V(start) of a given Target Policy
  using episodes generated by a (different) Behavior Policy. Reports both Ordinary
  and Weighted Importance Sampling estimates and plots their trajectories.
- Produces plots side-by-side so students can compare convergence behavior and variance.

Run:
    python slides/examples/mc_compare_on_off.py

Requirements:
    numpy, matplotlib

Author: Generated by assistant for teaching purposes.
"""

from collections import defaultdict
from typing import List, Tuple, Callable, Dict
import random
import numpy as np
import matplotlib.pyplot as plt
import math

# ---------------------------
# Environment: Gridworld
# ---------------------------


class GridWorld:
    """
    Simple deterministic Gridworld.

    Coordinates: (row, col) with (0,0) at top-left.
    Actions: 0=UP, 1=RIGHT, 2=DOWN, 3=LEFT
    """

    def __init__(
        self,
        n_rows: int = 5,
        n_cols: int = 5,
        start: Tuple[int, int] = None,
        goal: Tuple[int, int] = None,
        hole: Tuple[int, int] = None,
    ):
        self.n_rows = n_rows
        self.n_cols = n_cols
        self.start = start if start is not None else (n_rows - 1, 0)
        self.goal = goal if goal is not None else (0, n_cols - 1)
        self.hole = hole if hole is not None else (1, 1)
        self.state = self.start
        self.actions = [0, 1, 2, 3]

    def reset(self, start_state: Tuple[int, int] = None) -> Tuple[int, int]:
        if start_state is None:
            self.state = self.start
        else:
            self.state = start_state
        return self.state

    def step(self, action: int) -> Tuple[Tuple[int, int], float, bool]:
        r, c = self.state
        if action == 0:  # UP
            r2, c2 = max(0, r - 1), c
        elif action == 1:  # RIGHT
            r2, c2 = r, min(self.n_cols - 1, c + 1)
        elif action == 2:  # DOWN
            r2, c2 = min(self.n_rows - 1, r + 1), c
        elif action == 3:  # LEFT
            r2, c2 = r, max(0, c - 1)
        else:
            raise ValueError("Invalid action")

        self.state = (r2, c2)
        if self.state == self.goal:
            return self.state, 1.0, True
        if self.state == self.hole:
            return self.state, -1.0, True
        return self.state, 0.0, False

    def state_space(self) -> List[Tuple[int, int]]:
        return [(r, c) for r in range(self.n_rows) for c in range(self.n_cols)]

    def is_terminal(self, s: Tuple[int, int]) -> bool:
        return s == self.goal or s == self.hole

    def to_index(self, s: Tuple[int, int]) -> int:
        return s[0] * self.n_cols + s[1]

    def from_index(self, idx: int) -> Tuple[int, int]:
        return (idx // self.n_cols, idx % self.n_cols)


# ---------------------------
# Value Iteration (for ground truth)
# ---------------------------


def value_iteration(
    env: GridWorld, gamma: float = 1.0, theta: float = 1e-6
) -> np.ndarray:
    """Compute optimal state-value function for the episodic deterministic environment."""
    V = np.zeros((env.n_rows, env.n_cols))
    # Terminal states should have zero *future* value. The immediate reward for
    # reaching a terminal state is returned by the transition (r). To avoid
    # double-counting that terminal reward during the Bellman update, set the
    # terminal state's value to 0 and handle terminal transitions explicitly.
    V[env.goal] = 0.0
    V[env.hole] = 0.0

    def q_value_from_state(s: Tuple[int, int], a: int) -> float:
        # deterministic transition
        old_state = env.state
        env.state = s
        s2, r, _ = env.step(a)
        env.state = old_state
        # If the next state is terminal, its future (discounted) value is 0,
        # so the q-value should be just the immediate reward. Otherwise include
        # the discounted future value.
        if env.is_terminal(s2):
            return r
        return r + gamma * V[s2]

    while True:
        delta = 0.0
        for r in range(env.n_rows):
            for c in range(env.n_cols):
                s = (r, c)
                if env.is_terminal(s):
                    continue
                q_values = [q_value_from_state(s, a) for a in env.actions]
                v_new = max(q_values)
                delta = max(delta, abs(V[s] - v_new))
                V[s] = v_new
        if delta < theta:
            break
    return V


# ---------------------------
# Policies & episode generation
# ---------------------------


def epsilon_greedy_action(
    Q: Dict[Tuple[Tuple[int, int], int], float],
    state: Tuple[int, int],
    actions: List[int],
    epsilon: float,
) -> int:
    """Epsilon-greedy with random tie-breaking among maxima."""
    if random.random() < epsilon:
        return random.choice(actions)
    q_vals = [Q[(state, a)] for a in actions]
    max_val = max(q_vals)
    max_actions = [a for a, q in zip(actions, q_vals) if q == max_val]
    return random.choice(max_actions)


def generate_episode(
    env: GridWorld,
    policy_fn: Callable[[Tuple[int, int]], int],
    start_state: Tuple[int, int] = None,
    first_action: int = None,
) -> List[Tuple[Tuple[int, int], int, float]]:
    """
    Generate an episode following policy_fn.
    If start_state and first_action are provided, the episode begins from that (s,a).
    """
    episode = []
    if start_state is not None:
        env.reset(start_state)
        if first_action is not None:
            s = start_state
            a = first_action
            s2, r, done = env.step(a)
            episode.append((s, a, r))
            if done:
                return episode
    else:
        env.reset()

    while True:
        s = env.state
        a = policy_fn(s)
        s2, r, done = env.step(a)
        episode.append((s, a, r))
        if done:
            break
    return episode


# ---------------------------
# On-policy First-Visit MC Control
# ---------------------------


def on_policy_first_visit_mc_control(
    env: GridWorld,
    num_episodes: int = 5000,
    gamma: float = 1.0,
    epsilon: float = 0.2,
    epsilon_decay: float = 0.999,
    epsilon_min: float = 0.05,
    exploring_starts_prob: float = 0.0,
    optimistic_init: float = 0.0,
) -> Tuple[Dict, Dict, List[float]]:
    """
    Run First-Visit On-Policy Monte Carlo Control with epsilon-greedy policy.

    Returns:
      - Q: dict mapping (s,a) -> value
      - greedy_policy: dict mapping s -> a (final greedy policy)
      - v_start_history: list of V(start) estimates per episode (V=start estimated as max_a Q)
    """
    actions = env.actions
    # optimistic initialization helps exploration
    Q = defaultdict(lambda: float(optimistic_init))
    Counts = defaultdict(int)
    v_start_history = []

    current_epsilon = epsilon

    for ep in range(1, num_episodes + 1):
        # occasionally use exploring starts to ensure coverage
        if random.random() < exploring_starts_prob:
            non_terms = [s for s in env.state_space() if not env.is_terminal(s)]
            start_s = random.choice(non_terms)
            start_a = random.choice(actions)
            episode = generate_episode(
                env,
                lambda ss: epsilon_greedy_action(Q, ss, actions, current_epsilon),
                start_s,
                start_a,
            )
        else:
            episode = generate_episode(
                env, lambda ss: epsilon_greedy_action(Q, ss, actions, current_epsilon)
            )

        # First-visit returns, process backwards
        G = 0.0
        visited = set()
        for t in reversed(range(len(episode))):
            s, a, r = episode[t]
            G = gamma * G + r
            if (s, a) not in visited:
                visited.add((s, a))
                Counts[(s, a)] += 1
                n = Counts[(s, a)]
                # incremental sample-average update
                Q[(s, a)] = Q[(s, a)] + (G - Q[(s, a)]) / n

        # record V(start) estimate
        start_qs = [Q[(env.start, a)] for a in actions]
        v_start = float(max(start_qs))
        v_start_history.append(v_start)

        # decay epsilon
        current_epsilon = max(epsilon_min, current_epsilon * epsilon_decay)

    # derive greedy policy from final Q (random tie-break)
    greedy_policy = {}
    for s in env.state_space():
        if env.is_terminal(s):
            continue
        q_vals = [Q[(s, a)] for a in actions]
        max_val = max(q_vals)
        max_actions = [a for a, q in zip(actions, q_vals) if q == max_val]
        greedy_policy[s] = random.choice(max_actions)

    return Q, greedy_policy, v_start_history


# ---------------------------
# Off-policy MC Evaluation (Importance Sampling)
# ---------------------------


def off_policy_mc_evaluation(
    env: GridWorld,
    target_policy_fn: Callable[[Tuple[int, int]], int],
    behavior_policy_obj,
    num_episodes: int = 5000,
    gamma: float = 1.0,
    target_epsilon: float = 0.0,
    per_decision: bool = False,
):
    """
    Estimate V(start) of target_policy using episodes from behavior_policy_obj.

    Parameters:
      - target_epsilon: if > 0, treat target_policy_fn as the greedy action and
        convert it to an epsilon-soft (stochastic) target policy with probability
        (1 - target_epsilon) for the greedy action and remaining mass uniformly
        on other actions. This avoids zero weights when the target is deterministic.
      - per_decision: if True, compute per-decision (step-wise) ordinary IS estimates
        in addition to trajectory-level ordinary/weighted IS.

    Returns a dictionary with keys:
      - 'ordinary': list of ordinary trajectory IS estimates per episode (cumulative average)
      - 'weighted': list of weighted (normalized) trajectory IS estimates per episode
      - 'perdec_ordinary': list of per-decision ordinary IS cumulative averages (if requested)
      - 'weight_stats': dict with diagnostics: {'nonzero_frac':..., 'max_W':..., 'median_W':...}
    """
    ordinary = []
    weighted = []
    perdec = [] if per_decision else None
    cum_ordinary = 0.0
    cum_weighted_num = 0.0
    cum_weight = 0.0

    # helper: probability of action under (possibly stochastic) target policy
    def target_prob(s, a):
        greedy_a = target_policy_fn(s)
        if target_epsilon <= 0.0:
            return 1.0 if greedy_a == a else 0.0
        else:
            n_actions = getattr(behavior_policy_obj, "n_actions", 4)
            # distribute epsilon mass uniformly among all actions
            if a == greedy_a:
                return 1.0 - target_epsilon + target_epsilon / n_actions
            else:
                return target_epsilon / n_actions

    # diagnostics for weights
    all_weights = []

    for i in range(1, num_episodes + 1):
        episode = generate_episode(env, behavior_policy_obj)
        # trajectory-level discounted return
        G = 0.0
        for t, (_, _, r) in enumerate(episode):
            G += (gamma**t) * r

        # trajectory-level importance weight
        W = 1.0
        zeroed = False
        for s, a, _ in episode:
            pi = target_prob(s, a)
            b = behavior_policy_obj.prob(s, a)
            if b == 0.0:
                W = 0.0
                zeroed = True
                break
            W *= pi / b
        all_weights.append(W)

        # ordinary trajectory IS (cumulative average)
        cum_ordinary += W * G
        ordinary.append(cum_ordinary / i)

        # weighted trajectory IS (normalized by cumulative weight sum)
        cum_weight += W
        cum_weighted_num += W * G
        if cum_weight == 0.0:
            weighted.append(0.0)
        else:
            weighted.append(cum_weighted_num / cum_weight)

        # per-decision ordinary IS (if requested)
        if per_decision:
            # compute per-step cumulative ratio rho_t and accumulate rho_t * discounted reward at t
            perdec_val = 0.0
            rho = 1.0
            for t, (_, a, r) in enumerate(episode):
                pi_t = target_prob(episode[t][0], a)
                b_t = behavior_policy_obj.prob(episode[t][0], a)
                if b_t == 0.0:
                    rho = 0.0
                else:
                    rho *= pi_t / b_t
                perdec_val += rho * (gamma**t) * r
            # maintain cumulative average for per-decision estimator
            if len(perdec) == 0:
                perdec.append(perdec_val)
            else:
                perdec.append((perdec[-1] * (i - 1) + perdec_val) / i)

    # compute weight diagnostics
    weights_arr = np.array(all_weights)
    nonzero_frac = (
        float(np.count_nonzero(weights_arr) / len(weights_arr))
        if len(weights_arr) > 0
        else 0.0
    )
    max_W = float(np.max(weights_arr)) if len(weights_arr) > 0 else 0.0
    median_W = float(np.median(weights_arr)) if len(weights_arr) > 0 else 0.0
    stats = {"nonzero_frac": nonzero_frac, "max_W": max_W, "median_W": median_W}

    result = {"ordinary": ordinary, "weighted": weighted, "weight_stats": stats}
    if per_decision:
        result["perdec_ordinary"] = perdec
    return result


# ---------------------------
# Behavior policies for off-policy
# ---------------------------


class UniformBehavior:
    def __init__(self, n_actions: int = 4, seed: int = None):
        self.n_actions = n_actions
        if seed is not None:
            random.seed(seed)

    def __call__(self, s):
        return random.randrange(self.n_actions)

    def prob(self, s, a):
        return 1.0 / self.n_actions


class BiasedBehavior:
    """
    Example biased behavior: prefers LEFT and DOWN (arbitrary).
    Probability distribution is independent of state here for simplicity.
    """

    def __init__(
        self,
        p: Tuple[float, float, float, float] = (0.4, 0.2, 0.3, 0.1),
        seed: int = None,
    ):
        assert math.isclose(sum(p), 1.0), "Probabilities must sum to 1"
        self.p = p
        if seed is not None:
            random.seed(seed)

    def __call__(self, s):
        return np.random.choice([0, 1, 2, 3], p=self.p)

    def prob(self, s, a):
        return self.p[a]


# ---------------------------
# Utilities: plotting helpers
# ---------------------------


def q_to_v_grid(env: GridWorld, Q: Dict) -> np.ndarray:
    V = np.zeros((env.n_rows, env.n_cols))
    for r in range(env.n_rows):
        for c in range(env.n_cols):
            s = (r, c)
            if env.is_terminal(s):
                V[r, c] = 1.0 if s == env.goal else -1.0
            else:
                qvals = [Q[(s, a)] for a in env.actions]
                V[r, c] = float(max(qvals))
    return V


def plot_side_by_side(
    env: GridWorld,
    true_V: np.ndarray,
    V_onpolicy: np.ndarray,
    v_start_hist: List[float],
    ord_is: List[float],
    wt_is: List[float],
    title_suffix: str = "",
):
    fig, axes = plt.subplots(2, 3, figsize=(15, 8))
    fig.suptitle(
        "On-policy MC (left) vs Off-policy IS (right) " + title_suffix, fontsize=14
    )

    ax0 = axes[0, 0]
    im0 = ax0.imshow(true_V, cmap="coolwarm", vmin=-1.0, vmax=1.0)
    ax0.set_title("Ground-truth V*")
    plt.colorbar(im0, ax=ax0, fraction=0.046)

    ax1 = axes[0, 1]
    im1 = ax1.imshow(V_onpolicy, cmap="coolwarm", vmin=-1.0, vmax=1.0)
    ax1.set_title("On-policy: Estimated V (max_a Q)")
    plt.colorbar(im1, ax=ax1, fraction=0.046)

    # show difference heatmap
    ax2 = axes[0, 2]
    diff = V_onpolicy - true_V
    im2 = ax2.imshow(diff, cmap="bwr", vmin=-1.0, vmax=1.0)
    ax2.set_title("Error (On-policy - True)")
    plt.colorbar(im2, ax=ax2, fraction=0.046)

    # V(start) plot
    ax3 = axes[1, 0]
    episodes = np.arange(1, len(v_start_hist) + 1)
    ax3.plot(
        episodes, v_start_hist, color="blue", label="V(start) estimate (on-policy)"
    )
    ax3.axhline(true_V[env.start], color="k", linestyle="--", label="True V(start)")
    ax3.set_title("On-Policy V(start) over episodes")
    ax3.set_xlabel("Episode")
    ax3.set_ylabel("V(start)")
    ax3.legend()

    # Off-policy IS plots
    ax4 = axes[1, 1]
    if len(ord_is) > 0:
        ax4.plot(range(1, len(ord_is) + 1), ord_is, label="Ordinary IS", alpha=0.6)
    if len(wt_is) > 0:
        ax4.plot(range(1, len(wt_is) + 1), wt_is, label="Weighted IS", alpha=0.9)
    ax4.axhline(true_V[env.start], color="k", linestyle="--", label="True V(start)")
    ax4.set_title("Off-Policy IS estimates (V(start))")
    ax4.set_xlabel("Episode")
    ax4.set_ylabel("Estimated V(start)")
    ax4.legend()

    # Rightmost bottom: zoomed view of early episodes (variance)
    ax5 = axes[1, 2]
    zoom = min(200, len(ord_is))
    if zoom > 0:
        ax5.plot(range(1, zoom + 1), ord_is[:zoom], label="Ordinary IS", alpha=0.6)
        ax5.plot(range(1, zoom + 1), wt_is[:zoom], label="Weighted IS", alpha=0.9)
        ax5.axhline(true_V[env.start], color="k", linestyle="--")
        ax5.set_title(f"IS early episodes (first {zoom})")
        ax5.legend()
    else:
        ax5.text(0.2, 0.5, "No off-policy data", transform=ax5.transAxes)

    plt.tight_layout(rect=[0, 0.03, 1, 0.95])
    plt.show()


# ---------------------------
# Experiment / main routine
# ---------------------------


def main():
    random.seed(0)
    np.random.seed(0)

    # Environment configuration
    env = GridWorld(n_rows=5, n_cols=5, start=(4, 0), goal=(0, 4), hole=(1, 1))

    # Compute ground-truth via value iteration
    true_V = value_iteration(env, gamma=1.0)

    # On-policy Monte Carlo Control
    print("Running On-Policy First-Visit MC Control...")
    Q_on, greedy_policy, v_start_hist = on_policy_first_visit_mc_control(
        env,
        num_episodes=3000,
        gamma=1.0,
        epsilon=0.3,
        epsilon_decay=0.995,
        epsilon_min=0.05,
        exploring_starts_prob=0.05,
        optimistic_init=0.5,
    )
    V_onpolicy = q_to_v_grid(env, Q_on)

    # Off-policy evaluation setup:
    # Target policy: the final greedy policy learned by on-policy MC
    def target_policy_fn(s):
        # If s terminal, return arbitrary (not used)
        return greedy_policy.get(s, 0)

    # Behavior policy: uniform random (more exploration), or biased
    behavior = UniformBehavior(n_actions=4, seed=1)
    # behavior = BiasedBehavior(seed=1)  # try alternate behavior if desired

    print("Running Off-Policy IS evaluation (ordinary, weighted, per-decision)...")
    # Use a small target_epsilon to make the target policy stochastic (avoid zero weights)
    results = off_policy_mc_evaluation(
        env,
        target_policy_fn,
        behavior,
        num_episodes=3000,
        gamma=1.0,
        target_epsilon=0.05,
        per_decision=True,
    )
    ord_is = results["ordinary"]
    wt_is = results["weighted"]
    perdec_is = results.get("perdec_ordinary", [])
    weight_stats = results["weight_stats"]
    print(
        f"Off-policy weight diagnostics: nonzero_frac={weight_stats['nonzero_frac']:.4f}, max_W={weight_stats['max_W']:.4f}, median_W={weight_stats['median_W']:.4f}"
    )

    # Plot comparison
    plot_side_by_side(
        env, true_V, V_onpolicy, v_start_hist, ord_is, wt_is, title_suffix="(Gridworld)"
    )

    # Print summary statistics
    print("\nSummary:")
    print(f"True V(start) = {true_V[env.start]:.4f}")
    print(f"On-policy final V(start) = {v_start_hist[-1]:.4f}")
    if len(ord_is) > 0:
        print(f"Off-policy ordinary IS final estimate = {ord_is[-1]:.4f}")
    if len(wt_is) > 0:
        print(f"Off-policy weighted IS final estimate = {wt_is[-1]:.4f}")


if __name__ == "__main__":
    main()
