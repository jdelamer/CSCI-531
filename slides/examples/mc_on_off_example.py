#!/usr/bin/env python3
"""
Monte Carlo on-policy control and off-policy evaluation example.

This script demonstrates:
- On-policy first-visit Monte Carlo control with an epsilon-greedy policy.
- Off-policy Monte Carlo evaluation using Ordinary Importance Sampling (IS)
  and Weighted Importance Sampling (WIS).

Environment: simple deterministic Chain MDP
- States: 0..N-1
- Start: middle state
- Terminals: 0 (reward 0), N-1 (reward 1)
- Actions: 0=LEFT, 1=RIGHT

Run as a script. It will produce a couple of plots that illustrate:
- Improvement of episode returns under on-policy MC control.
- Behavior of ordinary and weighted IS when evaluating a deterministic target
  policy from episodes generated by a behavior policy.
"""

from collections import defaultdict
import random
import numpy as np
import matplotlib.pyplot as plt
from typing import Callable, List, Tuple, Iterable

# ---------------------------
# Environment
# ---------------------------


class ChainEnv:
    """
    Deterministic chain environment.

    States: 0 .. n-1
    Start state: n//2
    Terminal states: 0 (left, reward=0), n-1 (right, reward=1)
    Actions: 0 (LEFT), 1 (RIGHT)
    """

    def __init__(self, n_states: int = 5, seed: int = None):
        assert n_states >= 3, "Need at least 3 states"
        self.n = n_states
        self.start = n_states // 2
        self.left_terminal = 0
        self.right_terminal = n_states - 1
        self.state = self.start
        if seed is not None:
            random.seed(seed)
            np.random.seed(seed)

    def reset(self) -> int:
        self.state = self.start
        return self.state

    def step(self, action: int) -> Tuple[int, float, bool]:
        if action == 0:
            new_state = max(0, self.state - 1)
        else:
            new_state = min(self.n - 1, self.state + 1)

        reward = 0.0
        done = False
        if new_state == self.left_terminal:
            done = True
            reward = 0.0
        elif new_state == self.right_terminal:
            done = True
            reward = 1.0

        self.state = new_state
        return new_state, reward, done


# ---------------------------
# Policy utilities & classes
# ---------------------------


def epsilon_greedy_action(
    Q: defaultdict, state: int, n_actions: int = 2, epsilon: float = 0.1
) -> int:
    """
    Return an action using epsilon-greedy derived from Q.

    Tie-breaking among maximal actions is random to avoid
    deterministic bias when Q is initialized uniformly (e.g., zeros).
    """
    # Exploration
    if random.random() < epsilon:
        return random.randrange(n_actions)
    # Greedy selection with random tie-breaking:
    q_vals = [Q[(state, a)] for a in range(n_actions)]
    max_val = max(q_vals)
    max_actions = [a for a, q in enumerate(q_vals) if q == max_val]
    return random.choice(max_actions)


class UniformBehaviorPolicy:
    """Uniform random policy over actions."""

    def __init__(self, n_actions: int = 2, seed: int = None):
        self.n_actions = n_actions
        if seed is not None:
            random.seed(seed)

    def __call__(self, state: int) -> int:
        return random.randrange(self.n_actions)

    def prob(self, state: int, action: int) -> float:
        return 1.0 / self.n_actions


class BiasedBehaviorPolicy:
    """Biased policy: with probability p_left choose LEFT, else RIGHT."""

    def __init__(self, p_left: float = 0.7, seed: int = None):
        assert 0.0 <= p_left <= 1.0
        self.p_left = p_left
        if seed is not None:
            random.seed(seed)

    def __call__(self, state: int) -> int:
        return 0 if random.random() < self.p_left else 1

    def prob(self, state: int, action: int) -> float:
        return self.p_left if action == 0 else (1.0 - self.p_left)


class DeterministicTargetPolicy:
    """Deterministic target policy: always choose a fixed action (e.g. RIGHT)."""

    def __init__(self, action: int = 1):
        self.action = action

    def __call__(self, state: int) -> int:
        return self.action

    def prob(self, state: int, action: int) -> float:
        return 1.0 if action == self.action else 0.0


# ---------------------------
# Episode generation
# ---------------------------


def generate_episode(
    env: ChainEnv, policy_fn: Callable[[int], int]
) -> List[Tuple[int, int, float]]:
    """
    Generate an episode following policy_fn. Returns list of (s, a, r).
    """
    episode = []
    s = env.reset()
    while True:
        a = policy_fn(s)
        s2, r, done = env.step(a)
        episode.append((s, a, r))
        if done:
            break
    return episode


# ---------------------------
# On-policy first-visit MC control
# ---------------------------


def on_policy_mc_control(
    env: ChainEnv, num_episodes: int = 2000, gamma: float = 1.0, epsilon: float = 0.1
) -> Tuple[defaultdict, dict, List[float]]:
    """
    First-visit Monte Carlo control, on-policy with epsilon-greedy.
    Returns:
      - Q: defaultdict mapping (s,a) -> value
      - policy: greedy policy derived from Q (dict: state -> action)
      - episode_returns: list of episode returns (sum of rewards)
    """
    Q = defaultdict(float)  # Q[(s,a)]
    Returns = defaultdict(list)  # Returns[(s,a)] is a list of sample returns
    n_actions = 2

    def policy_fn(s):
        return epsilon_greedy_action(Q, s, n_actions=n_actions, epsilon=epsilon)

    episode_returns = []

    for ep in range(1, num_episodes + 1):
        episode = generate_episode(env, policy_fn)
        # compute G for each time step (discounted return from that time)
        G = 0.0
        visited = set()
        # traverse backwards to compute returns efficiently
        for t in reversed(range(len(episode))):
            s, a, r = episode[t]
            G = gamma * G + r
            if (s, a) not in visited:
                visited.add((s, a))
                Returns[(s, a)].append(G)
                Q[(s, a)] = float(np.mean(Returns[(s, a)]))
        episode_returns.append(sum(r for (_, _, r) in episode))

    # derive greedy policy from final Q (random tie-breaking for readability)
    policy = {}
    for s in range(env.n):
        if s in (env.left_terminal, env.right_terminal):
            continue
        q_vals = [Q[(s, a)] for a in range(2)]
        max_val = max(q_vals)
        max_actions = [a for a, q in enumerate(q_vals) if q == max_val]
        policy[s] = int(random.choice(max_actions))

    return Q, policy, episode_returns


# ---------------------------
# Off-policy MC evaluation (importance sampling)
# ---------------------------


def off_policy_mc_evaluation(
    env: ChainEnv,
    target_policy,
    behavior_policy,
    num_episodes: int = 5000,
    gamma: float = 1.0,
) -> Tuple[List[float], List[float]]:
    """
    Off-policy Monte Carlo evaluation of target_policy using episodes from behavior_policy.

    Returns two lists:
      - ordinary_is_estimates (per-episode cumulative average of W*G)
      - weighted_is_estimates (per-episode weighted IS estimate)
    Notes:
      - target_policy: object with __call__(s) and prob(s,a) OR a callable (deterministic)
      - behavior_policy: object with __call__(s) and prob(s,a)
    """
    ordinary_estimates = []
    weighted_estimates = []
    cumulative_ordinary = 0.0
    cumulative_weighted_numer = 0.0
    cumulative_weight = 0.0

    # helper to obtain pi(a|s)
    def target_prob(s: int, a: int) -> float:
        if hasattr(target_policy, "prob"):
            return target_policy.prob(s, a)
        else:
            # assume deterministic callable
            return 1.0 if target_policy(s) == a else 0.0

    for i in range(1, num_episodes + 1):
        episode = generate_episode(env, behavior_policy)
        # compute discounted return G for the episode
        G = 0.0
        for t, (_, _, r) in enumerate(episode):
            G += (gamma**t) * r

        # compute importance sampling ratio W = product_t pi(a_t|s_t) / b(a_t|s_t)
        W = 1.0
        for s, a, _ in episode:
            pi = target_prob(s, a)
            b = behavior_policy.prob(s, a)
            if b == 0:
                W = 0.0
                break
            # if target gives zero probability to an action taken under behavior,
            # then W becomes 0 and that episode contributes nothing
            W *= pi / b

        cumulative_ordinary += W * G
        ordinary_estimates.append(cumulative_ordinary / i)

        cumulative_weight += W
        cumulative_weighted_numer += W * G
        if cumulative_weight == 0:
            weighted_estimates.append(0.0)
        else:
            weighted_estimates.append(cumulative_weighted_numer / cumulative_weight)

    return ordinary_estimates, weighted_estimates


# ---------------------------
# Helper to get reference estimate under target policy
# ---------------------------


def run_target_mc(
    env: ChainEnv, target_policy_fn: Callable[[int], int], n: int = 2000
) -> Tuple[float, List[float]]:
    """
    Run Monte Carlo sampling under the target_policy to get a reference estimate of V(start).
    Returns (mean_return, returns_list).
    """
    returns = []
    for _ in range(n):
        ep = generate_episode(env, target_policy_fn)
        returns.append(sum(r for (_, _, r) in ep))
    return float(np.mean(returns)), returns


# ---------------------------
# Plotting utilities
# ---------------------------


def moving_average(x: Iterable[float], window: int = 50) -> np.ndarray:
    x = np.array(x, dtype=float)
    if len(x) < window:
        return x
    return np.convolve(x, np.ones(window) / window, mode="valid")


# ---------------------------
# Main demo
# ---------------------------


def main():
    SEED = 42
    env = ChainEnv(n_states=7, seed=SEED)

    # ---------------------
    # On-policy MC control
    # ---------------------
    print("Running on-policy MC control (first-visit, epsilon-greedy)...")
    Q, learned_policy, returns = on_policy_mc_control(
        env, num_episodes=2000, gamma=1.0, epsilon=0.1
    )
    print("Learned greedy policy (state -> action):")
    for s in sorted(learned_policy.keys()):
        print(f"  s={s} -> a={learned_policy[s]}")

    plt.figure(figsize=(9, 3))
    ma = moving_average(returns, window=50)
    plt.plot(ma, label="episode return (50-ep MA)")
    plt.title("On-policy MC control: Episode returns (moving avg)")
    plt.xlabel("Episode (50-ep MA index)")
    plt.ylabel("Return")
    plt.legend()
    plt.tight_layout()
    plt.show()

    # ---------------------
    # Off-policy evaluation
    # ---------------------
    print("\nRunning off-policy evaluation using Importance Sampling...")
    env2 = ChainEnv(n_states=7, seed=SEED + 1)
    target = DeterministicTargetPolicy(action=1)  # always go RIGHT
    behavior = UniformBehaviorPolicy(n_actions=2, seed=SEED + 2)

    # Ordinary and weighted IS estimates
    ord_est, wt_est = off_policy_mc_evaluation(
        env2, target, behavior, num_episodes=5000, gamma=1.0
    )

    # reference ground-truth estimate under target policy
    env3 = ChainEnv(n_states=7, seed=SEED + 3)
    true_mean, _ = run_target_mc(env3, target, n=2000)
    print(
        f"Reference Monte Carlo estimate of V(start) under target policy (2000 eps): {true_mean:.4f}"
    )
    print(
        f"Final IS estimates (after 5000 eps): ordinary={ord_est[-1]:.4f}, weighted={wt_est[-1]:.4f}"
    )

    plt.figure(figsize=(9, 4))
    # plot first 1000 for clarity
    max_plot = min(1000, len(ord_est))
    plt.plot(range(max_plot), ord_est[:max_plot], alpha=0.6, label="Ordinary IS")
    plt.plot(range(max_plot), wt_est[:max_plot], alpha=0.9, label="Weighted IS")
    plt.axhline(
        true_mean, color="k", linestyle="--", label="Reference V(start) under target"
    )
    plt.title(
        "Off-policy MC evaluation: IS estimates of V(start) (first 1000 episodes)"
    )
    plt.xlabel("Episodes")
    plt.ylabel("Estimated V(start)")
    plt.legend()
    plt.tight_layout()
    plt.show()


if __name__ == "__main__":
    main()
