{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "!pip install gymnasium\n",
    "!pip install plotly\n",
    "from myst_nb import glue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's practice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will see a very simle grid world problem. In this environment the agent start always in the same position and needs to reach a cell in the world.\n",
    "To complicate things for the agent, one cell is a trap and if the agent go on it the game is lost.\n",
    "\n",
    "Visualy the world will look like this:\n",
    "\n",
    "```{figure} /lectures/mdp/dynamic-programming/grid_world.png\n",
    ":align: center\n",
    ":width: 70%\n",
    "```\n",
    "\n",
    "Since MDPs (Markov Decision Processes) are designed to solve stochastic problems, we will introduce some stochasticity to the problem. Specifically, we will add uncertainty to the agent's movements. When the agent decides to move in one direction, such as UP, there is an 80% chance of moving in the desired direction and a 10% chance of moving to one of the adjacent cells.\n",
    "\n",
    "```{figure} /lectures/mdp/dynamic-programming/transition.png\n",
    ":align: center\n",
    ":width: 70%\n",
    "```\n",
    "\n",
    "If the agent moves into a wall, the agent stays in the same cell instead.\n",
    "\n",
    "## Markov Decision Process\n",
    "\n",
    "Before implementing the environment and solving it, we need to define it as a MDP $\\Omega = (S,A,T,R)$, where:\n",
    "- $S$ is the state space: each state $s$ will be a cell of the grid such as $\\forall s \\in S, s \\in [0, 11]$.\n",
    "- $A$ is the action space: each action $a$ will be a direction such as $\\forall a \\in A, a \\in \\{UP, DOWN, LEFT, RIGHT\\}$.\n",
    "- $T$ is the transition function. Moving in a specific direction have a succes chance of $80\\%$, and there is a $20\\%$ probability of going in the adjacent cells instead.\n",
    "- $R$ is the reward function: Entering the goal gives $+10$, falling in the trap gives $-10$, and all the other actions gives $0$.\n",
    "\n",
    "## Environment Implementation\n",
    "\n",
    "Again, we will implement the environment using `Gym` conventions and call it `GridWorld`.\n",
    "\n",
    "```python\n",
    "class GridWorld(gymnasium.Env):\n",
    "  def __init__(self):\n",
    "    pass\n",
    "\n",
    "  def step(self, action: int):\n",
    "    pass\n",
    "```\n",
    "\n",
    "### State space and Action space\n",
    "\n",
    "Based on our definition of the MDP, the state space represents all the states possible, which in this problem is all the cells the agent can reach, and the action space is all the action the agent can perform.\n",
    "\n",
    "In both cases the spaces are discrete, so we can use the `Discrete` type of `Gym` to represent them.\n",
    "\n",
    "```python\n",
    "self.action_space = spaces.Discrete(4) # Up, Down, Left, Right\n",
    "self.observation_space = spaces.Discrete(12) # 12 cells\n",
    "```\n",
    "\n",
    "```{margin} State space\n",
    "There is not one way to represent the state space for a problem.\n",
    "In this problem we could represent it by the grid itself where the agent will be represented by the cell having boolean equal to `True`.\n",
    "You would have `spaces.MultiDiscrete(np.array([3, 4]))`.\n",
    "```\n",
    "\n",
    "```{note}\n",
    "We consider the a state space containing 12 states, so it includes the wall cell. The agent cannot technically go on it, but it eases the implementation so we include it.\n",
    "```\n",
    "\n",
    "### Transition function\n",
    "\n",
    "In reinforcement learning, we don't need to explicitly implement the transition function. However, solving MDPs with value iteration requires it: \n",
    "\n",
    "```{math}\n",
    "V(s_{k+1}) = \\sum_a\\pi(a|s)\\sum_{s'}p(s'|s,a)\\left[ r(s,a) + \\gamma v_k(s') \\right]\n",
    "```\n",
    "\n",
    "`Gym` doesn't provide any method or convention of doing it, so we'll create a variable `P` representing our transiton function. It will be a dictionary of dictionary, where the first key is the current state and the second key will be the action. It will return the list of list containing the probability, the state and the reward.\n",
    "\n",
    "```python\n",
    "self.P = { 0: { 0: [[0.9, 0, 0], [0.1, 1, 0]], 1: [[0.8, 4, 0], [0.1, 1, 0], [0.1, 0, 0]], 2: [[0.9, 0, 0], [0.1, 4, 0]], 3: [[0.8, 1, 0], [0.1, 4, 0], [0.1, 0, 0]]},\n",
    "               1: { 0: [[0.8, 1, 0], [0.1, 0, 0], [0.1, 2, 0]],  1: [[0.8, 1, 0], [0.1, 0, 0], [0.1, 2, 0]], 2: [[0.8, 0, 0], [0.2, 1, 0]], 3: [[0.8, 2, 0], [0.2, 1, 0]]},\n",
    "               2: { 0: [[0.8, 2, 0], [0.1, 1, 0], [0.1, 3, 0]], 1: [[0.8, 6, 0], [0.1, 1, 0], [0.1, 3, 0]], 2: [[0.8, 1, 0], [0.1, 2, 0], [0.1, 6, 0]], 3: [[0.8, 3, 0], [0.1, 2, 0], [0.1, 6, 0]]},\n",
    "               3: { 0: [[0.9, 3, 0], [0.1, 2, 0]], 1: [[0.8, 7, -10], [0.1, 2, 0], [0.1, 3, 0]], 2: [[0.8, 2, 0], [0.1, 3, 0], [0.1, 7, -10]], 3: [[0.9, 3, 0], [0.1, 7, -10]]},\n",
    "               4: { 0: [[0.8, 0, 0], [0.2, 4, 0]], 1: [[0.8, 8, 0], [0.2, 4, 0]], 2: [[0.8, 4, 0], [0.1, 0, 0], [0.1, 8, 0]], 3: [[0.8, 4, 0], [0.1, 0, 0], [0.1, 8, 0]]},\n",
    "               5: { 0: [[1, 5, 0]], 1: [[1, 5, 0]], 2: [[1, 5, 0]], 3: [[1, 5, 0]]},\n",
    "               6: { 0: [[0.8, 2, 0], [0.1, 6, 0], [0.1, 7, -10]], 1: [[0.8, 10, 0], [0.1, 6, 0], [0.1, 7, -10]], 2: [[0.8, 6, 0], [0.1, 10, 0], [0.1, 2, 0]], 3: [[0.8, 7, -10], [0.1, 2, 0], [0.1, 10, 0]]},\n",
    "               7: { 0: [[1, 7, -10]], 1: [[1, 7, -10]], 2: [[1, 7, -10]], 3: [[1, 7, -10]]},\n",
    "               8: { 0: [[0.8, 4, 0], [0.1, 9, 0], [0.1, 8, 0]], 1: [[0.9, 8, 0], [0.1, 9, 0]], 2: [[0.9, 8, 0], [0.1, 4, 0]], 3: [[0.8, 9, 0], [0.1, 4, 0], [0.1, 8, 0]]},\n",
    "               9: { 0: [[0.8, 9, 0], [0.1, 8, 0], [0.1, 10, 0]], 1: [[0.8, 9, 0], [0.1, 8, 0], [0.1, 10, 0]], 2: [[0.8, 8, 0], [0.2, 9, 0]], 3: [[0.8, 10, 0], [0.2, 9, 0]]},\n",
    "               10: { 0: [[0.8, 6, 0], [0.1, 9, 0], [0.1, 11, 10]], 1: [[0.8, 10, 0], [0.1, 9, 0], [0.1, 11, 10]], 2: [[0.8, 9, 0], [0.1, 6, 0], [0.1, 10, 0]], 3: [[0.8, 11, 10], [0.1, 6, 0], [0.1, 10, 0]]},\n",
    "               11: { 0: [[1, 11, 10]], 1: [[1, 11, 10]], 2: [[1, 11, 10]], 3: [[1, 11, 10]]},\n",
    "              } \n",
    "```\n",
    "\n",
    "It is preferable to not use the variable `P` for applying the transition in the environment. We can implement it by modifying the action based on a random number to simulate the error. If the random number is below $0.8$, then it is a success and the action stay the same. Otherwise the action is changed to the clockwise or counter clockwise action.\n",
    "\n",
    "```{figure} /lectures/mdp/dynamic-programming/actions.png\n",
    ":align: center\n",
    ":width: 50%\n",
    "```\n",
    "\n",
    "```python\n",
    "prob = np.random.random()\n",
    "if prob < 0.80:\n",
    "  actual_action = action\n",
    "elif prob < 0.90:\n",
    "  # Adjacent cell \"clockwise\"\n",
    "  actual_action = (action + 1) % 4\n",
    "else:\n",
    "  # Adjacent cell \"counter clockwise\"\n",
    "  actual_action = (action - 1) % 4  \n",
    "```\n",
    "\n",
    "Our state is represented as an integer, but we can convert it to coordinates.\n",
    "\n",
    "```python\n",
    "r = np.floor(self.state / 3)\n",
    "c = self.state % 4\n",
    "```\n",
    "\n",
    "Once it is done we can apply the action and modify the coordinates.\n",
    "\n",
    "```python\n",
    "if actual_action == 0:\n",
    "  r = max(0, r - 1)\n",
    "elif actual_action == 2:\n",
    "  r = min(2, r + 1)\n",
    "elif actual_action == 1:\n",
    "  c = max(0, c - 1)\n",
    "elif actual_action == 3:\n",
    "  c = min(3, c + 1)\n",
    "```\n",
    " And finally, we convert it back to an integer.\n",
    "\n",
    "```python\n",
    "self.state = r * 4 + c\n",
    "```\n",
    "\n",
    "If we put everyting together, we obtain a function that will manage the transtion for us.\n",
    "\n",
    "```python\n",
    "\n",
    "  def _transition(self, action: int):\n",
    "    \"\"\"\n",
    "    Transition function.\n",
    "    :param action: Action to take\n",
    "    \"\"\"\n",
    "    r = np.floor(self.state / 3)\n",
    "    c = self.state % 4\n",
    "\n",
    "    prob = np.random.random()\n",
    "    if prob < 0.80:\n",
    "      actual_action = action\n",
    "    elif prob < 0.90:\n",
    "      # Adjacent cell \"clockwise\"\n",
    "      actual_action = (action + 1) % 4\n",
    "    else:\n",
    "      # Adjacent cell \"counter clockwise\"\n",
    "      actual_action = (action - 1) % 4\n",
    "\n",
    "\n",
    "    if actual_action == 0:\n",
    "      r = max(0, r - 1)\n",
    "    elif actual_action == 2:\n",
    "      r = min(2, r + 1)\n",
    "    elif actual_action == 1:\n",
    "      c = max(0, c - 1)\n",
    "    elif actual_action == 3:\n",
    "      c = min(3, c + 1)\n",
    "    self.state = r * 4 + c\n",
    "```\n",
    "\n",
    "### Reward function\n",
    "\n",
    "The reward is straightforward, if we reach the $+10$ ($-10$) cell the agent receive $+10$ ($-10$), otherwise the rewards is always $0$.\n",
    "\n",
    "We implement the reward directly in the `step` function.\n",
    "\n",
    "```python\n",
    "\n",
    "  def step(self, action: int):\n",
    "    self._transition(action)\n",
    "    \n",
    "    done = False\n",
    "\n",
    "    reward = 0\n",
    "    if self.state == 11:\n",
    "      reward = 10\n",
    "      done = True\n",
    "    elif self.state == 7:\n",
    "      reward = -10\n",
    "      done = True\n",
    "\n",
    "    # Return the observation, reward, done flag, and info\n",
    "    return self.state, reward, done, {}\n",
    "```\n",
    "\n",
    "### Everything together\n",
    "\n",
    "The full envronment is provided below. You will see it contains a way to render the environment, it can be useful sometimes.\n",
    "\n",
    "```{admonition} Activity\n",
    ":class: activity\n",
    "\n",
    "The function `render` doesn't show the position of the agent and it is up to you to finish it.\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "import gymnasium\n",
    "from gymnasium import spaces\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Rectangle\n",
    "\n",
    "class GridWorld(gymnasium.Env):\n",
    "  def __init__(self):\n",
    "    # Define the action and observation spaces\n",
    "    self.action_space = spaces.Discrete(4) # Up, Down, Left, Right\n",
    "    self.observation_space = spaces.Discrete(12) # 12 cells\n",
    "\n",
    "    self.P = { 0: { 0: [[0.9, 0, 0], [0.1, 1, 0]], 1: [[0.8, 4, 0], [0.1, 1, 0], [0.1, 0, 0]], 2: [[0.9, 0, 0], [0.1, 4, 0]], 3: [[0.8, 1, 0], [0.1, 4, 0], [0.1, 0, 0]]},\n",
    "               1: { 0: [[0.8, 1, 0], [0.1, 0, 0], [0.1, 2, 0]],  1: [[0.8, 1, 0], [0.1, 0, 0], [0.1, 2, 0]], 2: [[0.8, 0, 0], [0.2, 1, 0]], 3: [[0.8, 2, 0], [0.2, 1, 0]]},\n",
    "               2: { 0: [[0.8, 2, 0], [0.1, 1, 0], [0.1, 3, 0]], 1: [[0.8, 6, 0], [0.1, 1, 0], [0.1, 3, 0]], 2: [[0.8, 1, 0], [0.1, 2, 0], [0.1, 6, 0]], 3: [[0.8, 3, 0], [0.1, 2, 0], [0.1, 6, 0]]},\n",
    "               3: { 0: [[0.9, 3, 0], [0.1, 2, 0]], 1: [[0.8, 7, -10], [0.1, 2, 0], [0.1, 3, 0]], 2: [[0.8, 2, 0], [0.1, 3, 0], [0.1, 7, -10]], 3: [[0.9, 3, 0], [0.1, 7, -10]]},\n",
    "               4: { 0: [[0.8, 0, 0], [0.2, 4, 0]], 1: [[0.8, 8, 0], [0.2, 4, 0]], 2: [[0.8, 4, 0], [0.1, 0, 0], [0.1, 8, 0]], 3: [[0.8, 4, 0], [0.1, 0, 0], [0.1, 8, 0]]},\n",
    "               5: { 0: [[1, 5, 0]], 1: [[1, 5, 0]], 2: [[1, 5, 0]], 3: [[1, 5, 0]]},\n",
    "               6: { 0: [[0.8, 2, 0], [0.1, 6, 0], [0.1, 7, -10]], 1: [[0.8, 10, 0], [0.1, 6, 0], [0.1, 7, -10]], 2: [[0.8, 6, 0], [0.1, 10, 0], [0.1, 2, 0]], 3: [[0.8, 7, -10], [0.1, 2, 0], [0.1, 10, 0]]},\n",
    "               7: { 0: [[1, 7, -10]], 1: [[1, 7, -10]], 2: [[1, 7, -10]], 3: [[1, 7, -10]]},\n",
    "               8: { 0: [[0.8, 4, 0], [0.1, 9, 0], [0.1, 8, 0]], 1: [[0.9, 8, 0], [0.1, 9, 0]], 2: [[0.9, 8, 0], [0.1, 4, 0]], 3: [[0.8, 9, 0], [0.1, 4, 0], [0.1, 8, 0]]},\n",
    "               9: { 0: [[0.8, 9, 0], [0.1, 8, 0], [0.1, 10, 0]], 1: [[0.8, 9, 0], [0.1, 8, 0], [0.1, 10, 0]], 2: [[0.8, 8, 0], [0.2, 9, 0]], 3: [[0.8, 10, 0], [0.2, 9, 0]]},\n",
    "               10: { 0: [[0.8, 6, 0], [0.1, 9, 0], [0.1, 11, 10]], 1: [[0.8, 10, 0], [0.1, 9, 0], [0.1, 11, 10]], 2: [[0.8, 9, 0], [0.1, 6, 0], [0.1, 10, 0]], 3: [[0.8, 11, 10], [0.1, 6, 0], [0.1, 10, 0]]},\n",
    "               11: { 0: [[1, 11, 10]], 1: [[1, 11, 10]], 2: [[1, 11, 10]], 3: [[1, 11, 10]]},\n",
    "              } \n",
    "\n",
    "    # Initialize the state\n",
    "    self.state = 0\n",
    "\n",
    "  def step(self, action: int):\n",
    "\n",
    "    self._transition(action)\n",
    "\n",
    "    done = False\n",
    "\n",
    "    reward = 0\n",
    "    if self.state == 11:\n",
    "      reward = 10\n",
    "      done = True\n",
    "    elif self.state == 7:\n",
    "      reward = -10\n",
    "      done = True\n",
    "\n",
    "\n",
    "    # Return the observation, reward, done flag, and info\n",
    "    return self.state, reward, done, {}\n",
    "\n",
    "  def _transition(self, action: int):\n",
    "    \"\"\"\n",
    "    Transition function.\n",
    "    :param action: Action to take\n",
    "    \"\"\"\n",
    "    r = np.floor(self.state / 3)\n",
    "    c = self.state % 4\n",
    "\n",
    "    prob = np.random.random()\n",
    "    if prob < 0.80:\n",
    "      actual_action = action\n",
    "    elif prob < 0.90:\n",
    "      # Adjacent cell \"clockwise\"\n",
    "      actual_action = (action + 1) % 4\n",
    "    else:\n",
    "      # Adjacent cell \"counter clockwise\"\n",
    "      actual_action = (action - 1) % 4\n",
    "\n",
    "\n",
    "    if actual_action == 0:\n",
    "      r = max(0, r - 1)\n",
    "    elif actual_action == 2:\n",
    "      r = min(2, r + 1)\n",
    "    elif actual_action == 1:\n",
    "      c = max(0, c - 1)\n",
    "    elif actual_action == 3:\n",
    "      c = min(3, c + 1)\n",
    "    self.state = r * 4 + c\n",
    "\n",
    "  def reset(self):\n",
    "    \"\"\"\n",
    "    Reset the environment.\n",
    "    \"\"\"\n",
    "    self.state = 0\n",
    "    return self.state\n",
    "\n",
    "  def render(self, render=\"human\"):\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.set_xlim(0, 4)\n",
    "    ax.set_ylim(0, 3)\n",
    "    ax.set_aspect('equal')\n",
    "\n",
    "\n",
    "    for i in range(4):\n",
    "      for j in range(3):\n",
    "        if j * 4 + i == 11:\n",
    "          rect = Rectangle((i, j), 1, 1, edgecolor='black', facecolor='green')\n",
    "          ax.add_patch(rect)\n",
    "        elif j * 4 + i == 7:\n",
    "          rect = Rectangle((i, j), 1, 1, edgecolor='black', facecolor='red')\n",
    "          ax.add_patch(rect)\n",
    "        elif j * 4 + i == 5:\n",
    "          rect = Rectangle((i, j), 1, 1, edgecolor='black', facecolor='grey')\n",
    "          ax.add_patch(rect)\n",
    "        else:\n",
    "          rect = Rectangle((i, j), 1, 1, edgecolor='black', facecolor='white')\n",
    "          ax.add_patch(rect)\n",
    "\n",
    "    ax.tick_params(axis='both',       # changes apply to both axis\n",
    "                    which='both',      # both major and minor ticks are affected\n",
    "                    bottom=False,      # ticks along the bottom edge are off\n",
    "                    top=False,         # ticks along the top edge are off\n",
    "                    left=False,\n",
    "                    right=False,\n",
    "                    labelbottom=False,\n",
    "                    labelleft=False) # labels along the bottom edge are off\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Value Iteration\n",
    "\n",
    "Value Iteration is very easy to implement, and take advantage of the information provide by the environment to simplify the code.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration(env, gamma=0.9, theta=0.0001):\n",
    "  V = np.zeros(env.observation_space.n)\n",
    "  while True:\n",
    "    delta = 0\n",
    "    for s in range(env.observation_space.n):\n",
    "      v = V[s]\n",
    "      V[s] = max([sum([p * (r + gamma * V[s_]) for p, s_, r in env.P[s][a]]) for a in env.P[s]])\n",
    "      delta = max(delta, np.abs(v - V[s]))\n",
    "      V[7] = -10\n",
    "      V[11] = 10\n",
    "    if delta < theta:\n",
    "      break\n",
    "  pi = np.zeros(env.observation_space.n)\n",
    "  for s in range(env.observation_space.n):\n",
    "    pi[s] = np.argmax([sum([p * (r + gamma * V[s_]) for p, s_, r in env.P[s][a]]) for a in env.P[s]])\n",
    "  return V, pi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the observation space to intialize the value function with zeros.\n",
    "\n",
    "```python\n",
    "V = np.zeros(env.observation_space.n)\n",
    "```\n",
    "\n",
    "We update the value for each state by looping other them (using the observation space again) and we apply the formula. This is where we actually need `P`, because it simplifies the code a lot.\n",
    "\n",
    "```python\n",
    "V[s] = max([sum([p * (r + gamma * V[s_]) for p, s_, r in env.P[s][a]]) for a in env.P[s]])\n",
    "```\n",
    "```{margin} Theta\n",
    "The value of theta is up to you, but if theta is too big yo may not converge to the optimal policy.\n",
    "Too small, it could take a long time to converge.\n",
    "```\n",
    "\n",
    "Finally, once we converged (based on $\\theta$) we calculate the policy the same way we calculated the value function but we use `argmax` instead.\n",
    "\n",
    "```python\n",
    "for s in range(env.observation_space.n):\n",
    "    pi[s] = np.argmax([sum([p * (r + gamma * V[s_]) for p, s_, r in env.P[s][a]]) for a in env.P[s]])\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "from IPython.display import display, clear_output\n",
    "\n",
    "def value_iteration_interact(env, gamma=0.9, theta=0.0001, step=0):\n",
    "  V = np.zeros(env.observation_space.n)\n",
    "  for i in range(step):\n",
    "    delta = 0\n",
    "    for s in range(env.observation_space.n):\n",
    "      v = V[s]\n",
    "      V[s] = max([sum([p * (r + gamma * V[s_]) for p, s_, r in env.P[s][a]]) for a in env.P[s]])\n",
    "      delta = max(delta, np.abs(v - V[s]))\n",
    "      V[7] = -10\n",
    "      V[11] = 10\n",
    "    if delta < theta:\n",
    "      break\n",
    "  pi = np.zeros(env.observation_space.n)\n",
    "  for s in range(env.observation_space.n):\n",
    "    pi[s] = np.argmax([sum([p * (r + gamma * V[s_]) for p, s_, r in env.P[s][a]]) for a in env.P[s]])\n",
    "  return V, pi\n",
    "\n",
    "def render(labels):\n",
    "  fig, ax = plt.subplots()\n",
    "  ax.set_xlim(0, 4)\n",
    "  ax.set_ylim(0, 3)\n",
    "  ax.set_aspect('equal')\n",
    "\n",
    "  for i in range(4):\n",
    "    for j in range(3):\n",
    "      if j * 4 + i == 11:\n",
    "        rect = Rectangle((i, j), 1, 1, edgecolor='black', facecolor='green')\n",
    "        ax.add_patch(rect)\n",
    "      elif j * 4 + i == 7:\n",
    "        rect = Rectangle((i, j), 1, 1, edgecolor='black', facecolor='red')\n",
    "        ax.add_patch(rect)\n",
    "      elif j * 4 + i == 5:\n",
    "        rect = Rectangle((i, j), 1, 1, edgecolor='black', facecolor='grey')\n",
    "        ax.add_patch(rect)\n",
    "      else:\n",
    "        rect = Rectangle((i, j), 1, 1, edgecolor='black', facecolor='white')\n",
    "        ax.add_patch(rect)\n",
    "      ax.text(i + 0.5, j + 0.5, int(labels[j * 4 + i]), ha='center', va='center')\n",
    "\n",
    "  ax.tick_params(axis='both',        # changes apply to both axis\n",
    "                  which='both',      # both major and minor ticks are affected\n",
    "                  bottom=False,      # ticks along the bottom edge are off\n",
    "                  top=False,         # ticks along the top edge are off\n",
    "                  left=False,\n",
    "                  right=False,\n",
    "                  labelbottom=False,\n",
    "                  labelleft=False)   # labels along the bottom edge are off\n",
    "  display(fig)\n",
    "  plt.close(fig)\n",
    "\n",
    "def update(gamma=0.9, step=0):\n",
    "  V, pi = value_iteration_interact(env, gamma, step=step)\n",
    "  render(V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "from ipywidgets import *\n",
    "\n",
    "env = GridWorld()\n",
    "interact(update, gamma = (0.5,0.9,0.1), step = (0, 50, 1));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "import numpy as np\n",
    "\n",
    "def value_iteration_interact(env, gamma=0.9, theta=0.0001, step=0):\n",
    "  V = np.zeros(env.observation_space.n)\n",
    "  V[7] = -10\n",
    "  V[11] = 10\n",
    "  for i in range(step):\n",
    "    delta = 0\n",
    "    for s in range(env.observation_space.n):\n",
    "      v = V[s]\n",
    "      V[s] = max([sum([p * (r + gamma * V[s_]) for p, s_, r in env.P[s][a]]) for a in env.P[s]])\n",
    "      delta = max(delta, np.abs(v - V[s]))\n",
    "      V[7] = -10\n",
    "      V[11] = 10\n",
    "    if delta < theta:\n",
    "      break\n",
    "  pi = np.zeros(env.observation_space.n)\n",
    "  for s in range(env.observation_space.n):\n",
    "    pi[s] = np.argmax([sum([p * (r + gamma * V[s_]) for p, s_, r in env.P[s][a]]) for a in env.P[s]])\n",
    "  return V, pi\n",
    "\n",
    "\n",
    "env = GridWorld()\n",
    "# Create figure\n",
    "fig = go.Figure()\n",
    "\n",
    "# Add traces, one for each slider step\n",
    "for step in range(0, 15):\n",
    "    V, _ = value_iteration_interact(env, 0.9, step=step) \n",
    "    x, y, text = [], [], []\n",
    "    for i in range(4):\n",
    "      for j in range(3):\n",
    "        if j * 4 + i == 11:\n",
    "          fig.add_shape(type=\"rect\", x0=i, y0=j, x1=i + 1, y1=j + 1, line=dict(color=\"black\"), fillcolor=\"green\", layer=\"below\")\n",
    "        elif j * 4 + i == 7:\n",
    "          fig.add_shape(type=\"rect\", x0=i, y0=j, x1=i + 1, y1=j + 1, line=dict(color=\"black\"), fillcolor=\"red\", layer=\"below\")\n",
    "        elif j * 4 + i == 5:\n",
    "          fig.add_shape(type=\"rect\", x0=i, y0=j, x1=i + 1, y1=j + 1, line=dict(color=\"black\"), fillcolor=\"gray\", layer=\"below\")\n",
    "        else:\n",
    "          fig.add_shape(type=\"rect\", x0=i, y0=j, x1=i + 1, y1=j + 1, line=dict(color=\"black\"), fillcolor=\"white\", layer=\"below\")\n",
    "        x.append(i + 0.5)\n",
    "        y.append(j + 0.5)\n",
    "        text.append(int(V[j * 4 + i]))\n",
    "    fig.add_trace(go.Scatter(\n",
    "      x=x,\n",
    "      y=y,\n",
    "      text=text,\n",
    "      mode=\"text\",\n",
    "      textfont=dict(\n",
    "          color=\"black\",\n",
    "          size=18,\n",
    "          family=\"Arail\",\n",
    "      ),\n",
    "      visible=False\n",
    "    )\n",
    ")\n",
    "fig.data[0].visible = True\n",
    "# Create and add slider\n",
    "steps = []\n",
    "for i in range(len(fig.data)):\n",
    "    step = dict(\n",
    "        method=\"update\",\n",
    "        args=[{\"visible\": [False] * len(fig.data)}],  # layout attribute\n",
    "    )\n",
    "    step[\"args\"][0][\"visible\"][i] = True  # Toggle i'th trace to \"visible\"\n",
    "    steps.append(step)\n",
    "\n",
    "sliders = [dict(\n",
    "    active=0,\n",
    "    pad={\"t\": 50},\n",
    "    steps=steps\n",
    ")]\n",
    "\n",
    "fig.update_xaxes(\n",
    "    showticklabels=False,\n",
    "    showgrid=False,\n",
    "    zeroline=False,\n",
    "    automargin=True\n",
    ")\n",
    "\n",
    "fig.update_yaxes(\n",
    "    showticklabels=False,\n",
    "    showgrid=False,\n",
    "    zeroline=False,\n",
    "    scaleanchor=\"x\",\n",
    "    scaleratio=1,\n",
    "    automargin=True\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    autosize=False,\n",
    "    sliders=sliders\n",
    ")\n",
    "\n",
    "fig.show()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
