{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's practice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "from myst_nb import glue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gym environment\n",
    "\n",
    "It is time to practice what we have seen so far. We explained before, that the agent interacts with an environment.\n",
    "In this chapter, the environment is the multi-armed bandit problem. If we want to train an agent, we will need to implement the environment.\n",
    "\n",
    "In our case we will implement the environment using the library Gymnasium (or Gym).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will start by importing the library we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from gymnasium.spaces import Discrete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gym environment.\n",
    "\n",
    "Gym environments have many built-in methods we can override, but for now we will only need a few.\n",
    "\n",
    "- Every `gym` environment must inherit from the class `Env`.\n",
    "- `__init__` initialize the environment: the constructor.\n",
    "- The method `step` execute the action sent by the agent.\n",
    "\n",
    "All your environment will looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove-output"
    ]
   },
   "outputs": [],
   "source": [
    "class MyEnv(gym.Env):\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "        #  Initialize the variables you need.\n",
    "    \n",
    "    def step(self, action):\n",
    "        pass\n",
    "        #  Execute an action."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we create the environment as any other object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_env = MyEnv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-armed Bandit\n",
    "\n",
    "Now we can implement the multi-armed bandit. The code is very simple, but it will illustrate how environment are implemented.\n",
    "\n",
    "### Constructor\n",
    "\n",
    "In the constructor we will define:\n",
    "- the number of actions\n",
    "- how they will behave\n",
    "\n",
    "In the bandit problem, each action have a hidden value. We could define them manually or in this example we can generate them randomly.\n",
    "\n",
    "Let's see the code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def __init__(self, nb_actions):\n",
    "    self.action_space = Discrete(nb_actions)\n",
    "    self.q_a = np.random.default_rng().normal(0, 1, nb_actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can notice a few things:\n",
    "- `Discrete` is a \"type\" in the `gym` library that defines a variable that can take discrete values. In this case between 0 and `nb_actions`.\n",
    "- `np.random.default_rng().normal(0, 1, nb_actions)` generates a list of `nb_actions` values between 0 and 1.\n",
    "\n",
    "```{note}\n",
    "We name the list `q_a` because we are generating the expected rewards $q(a)$.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Executing the actions\n",
    "\n",
    "Once the actions are defined, the method `step` can be implemented.\n",
    "\n",
    "In the case of the bandit problem, we just consider the action selected by the agent, the button pressed, and return the value associated to it.\n",
    "\n",
    "If you remember, the value returned is not the expected reward $q(a)$, but a value selected from a probability distribution. To make sure it converges to the $q(a)$ defined before, we will generate a value from a normal distribution centered on $q(a)$.\n",
    "\n",
    "Let's look at the code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step(self, action):\n",
    "    return np.random.default_rng().normal(self.q_a[action], 1, 1)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can notice that we use `q_a` to center the normal distribution, and we use a variance of 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting everything together\n",
    "\n",
    "We can create class `Bandit` that we generate a $N$ multi-armed bandit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bandit(gym.Env):\n",
    "\n",
    "    def __init__(self, nb_actions):\n",
    "        self.action_space = Discrete(nb_actions)\n",
    "        self.q_a = np.random.normal(0, 1, 10)\n",
    "    \n",
    "    def step(self, action):\n",
    "        return np.random.normal(self.q_a[action], 1, 1)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a bandit with 10 arms:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bandit = Bandit(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can print the expected value of each actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_a = bandit.q_a\n",
    "print(q_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "glue(\"q_a\", \"%.2f\" % q_a[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now if push button `0`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rew = bandit.step(0)\n",
    "print(rew)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "glue(\"rew_0\", \"%.2f\" % rew)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{important}\n",
    "Notice how the returned value {glue:}`rew_0` is different from the expected reward {glue:}`q_a`!\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solving the problem\n",
    "\n",
    "Now that the environment is implemented we can focus on solving the problem.\n",
    "\n",
    "### The Algorithm\n",
    "\n",
    "The algorithm will use the $\\epsilon$-greedy action selection, so we will call the function `e_greedy`.\n",
    "\n",
    "The parameters are simple:\n",
    "- The bandit environment\n",
    "- `\\espilon`: required by action selection strategy\n",
    "- A parameter `T`: the maximum number of iteration.\n",
    "\n",
    "It gives us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def e_greedy(bandit, e, T):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialization\n",
    "\n",
    "The agent learns the expected reward of each action $Q(a)$. To initialize this, we use two lists: `q_a` for saving current estimations, which are calculated by averaging the rewards received for each action. Additionally, we maintain `n_a` to track how many times each action has been selected, enabling us to apply the incremental formula.\n",
    "\n",
    "```{margin} Action Space\n",
    "By keeping the number of actions `bandit.action_space.n` in the environment we can create algorithm that are more general.\n",
    "\n",
    "Later in the course we will come back on the notion of action space.\n",
    "```\n",
    "\n",
    "```python\n",
    "q_a = [0 for i in range(bandit.action_space.n)]\n",
    "n_a = [0 for i in range(bandit.action_space.n)]\n",
    "```\n",
    "\n",
    "#### Action Selection\n",
    "\n",
    "The implementation is simple, we sample a random number. If the number is above $epsilon$ we choose the greedy action using the current estimate `q_a`. Otherwise we pick an action randomly.\n",
    "\n",
    "```python\n",
    "ran = np.random.random()\n",
    "a = 0\n",
    "if ran > e:\n",
    "    a = np.argmax(q_a)\n",
    "else:\n",
    "    a = np.random.randint(len(n_a))\n",
    "```\n",
    "\n",
    "#### Estimation Update\n",
    "\n",
    "Once we have the action we can simply execute the action and update the estimates with the reward received.\n",
    "\n",
    "```python\n",
    "r = bandit.step(a)\n",
    "n_a[a] += 1 \n",
    "q_a[a] += 1/n_a[a]*(r - q_a[a])\n",
    "```\n",
    "\n",
    "#### Full Algorithm\n",
    "\n",
    "Now we put everything together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def e_greedy(bandit, e, T):\n",
    "    q_a = [0 for i in range(bandit.action_space.n)]\n",
    "    n_a = [0 for i in range(bandit.action_space.n)]\n",
    "\n",
    "    for t in range(T):\n",
    "        ran = np.random.random()\n",
    "        a = 0\n",
    "        if ran > e:\n",
    "            a = np.argmax(q_a)\n",
    "        else:\n",
    "            a = np.random.randint(len(n_a))\n",
    "        r = bandit.step(a)\n",
    "        n_a[a] += 1 \n",
    "        q_a[a] += 1/n_a[a]*(r - q_a[a])\n",
    "\n",
    "    best_action = np.argmax(q_a)\n",
    "    return best_action, q_a[best_action]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We just added the variable `best_action` that will be returned at the end, and its estimated value. If the algorithm work we should get the best action and a got approximation.\n",
    "\n",
    "### Let's dot it!\n",
    "\n",
    "Now we have everything: the environment and the algorithm.\n",
    "\n",
    "Before launching the algorithm let's check what is the best action in the environment we created before.\n",
    "\n",
    "\n",
    "```{margin} Best Action\n",
    "We can get the best action because we generate it ourselves.\n",
    "It is usually not the case for most problems.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_action = np.argmax(bandit.q_a)\n",
    "print(\"Best action : {}, with Q(a): {}\".format(best_action, bandit.q_a[best_action]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we know the best action, we can run our training and see what is returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "np.random.seed(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_action_training, estimated_value = e_greedy(bandit, 0.2, 1000)\n",
    "print(\"Best action : {}, with Q(a): {}\".format(best_action_training, estimated_value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Analysis\n",
    "\n",
    "Before wrapping up this topic, it is important to discuss the training itself.\n",
    "\n",
    "In the previous example we trained for `1000` steps, and for this problem it worked very well. However, the number of steps necessary to converge varies depending of the problem.\n",
    "\n",
    "To visualize the impact of the training we will modify the previous algorithm to get more information:\n",
    "- The number of time the optimal action was picked.\n",
    "- The evolution of the expected reward.\n",
    "\n",
    "```{important}\n",
    "The expected reward is the expected reward of the best action. So it can vary a lot during training until it gets a good estimation of the each action's expected value.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def e_greedy(bandit, e, T):\n",
    "    q_a = [0 for i in range(bandit.action_space.n)]\n",
    "    n_a = [0 for i in range(bandit.action_space.n)]\n",
    "    e_r = []\n",
    "    best_action = np.argmax(bandit.q_a)\n",
    "    opt_action = []\n",
    "    opt_action_perc = []\n",
    "\n",
    "    for t in range(T):\n",
    "        ran = np.random.default_rng().random()\n",
    "        a = 0\n",
    "        if ran > e:\n",
    "            a = np.argmax(q_a)\n",
    "        else:\n",
    "            a = np.random.default_rng().integers(len(n_a))\n",
    "        r = bandit.step(a)\n",
    "        n_a[a] += 1 \n",
    "        q_a[a] += 1/n_a[a]*(r - q_a[a])\n",
    "        e_r.append(np.max(q_a))\n",
    "        if a == best_action:\n",
    "            opt_action.append(1)\n",
    "        else:\n",
    "            opt_action.append(0)\n",
    "        opt_action_perc.append(sum(opt_action)/len(opt_action)*100)\n",
    "\n",
    "    return e_r, opt_action_perc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The new information returned will be plotted to visualize the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_bandit(e_r, opt_action, T):\n",
    "    plt.style.use('seaborn-v0_8')\n",
    "    x = np.linspace(0, T, len(e_r))\n",
    "\n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1)\n",
    "    plt.subplots_adjust(hspace=0.5)\n",
    "\n",
    "    ax1.plot(x, e_r, linewidth=2.0, color=\"C1\")\n",
    "    ax1.set_title(\"Evolution of the Expected Reward\")\n",
    "    ax1.set_ylabel(\"Expected Reward\")\n",
    "    ax1.set_xlabel(\"Episodes\")\n",
    "\n",
    "    ax2.plot(x, opt_action, linewidth=2.0, color=\"C0\")\n",
    "    ax2.set_title(\"Evolution of Optimal Action\")\n",
    "    ax2.set_ylabel(\"%\")\n",
    "    ax2.set_xlabel(\"Episodes\")\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "np.random.seed(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e_r, opt_action_perc = e_greedy(bandit, 0.2, 1000)\n",
    "plot_bandit(e_r, opt_action_perc, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two things is hapenning with these graphs:\n",
    "- The expected reward varied a lot at the beginning, but stabilize quickly.\n",
    "- The percentage of optimal action stabilized after a few hundred steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
