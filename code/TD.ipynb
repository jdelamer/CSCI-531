{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "import gymnasium\n",
    "from gymnasium import spaces\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Rectangle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applying our first RL algorithms!\n",
    "\n",
    "## Grid World\n",
    "\n",
    "Let's go back to our grid world problem, where the agent start always in the same position and needs to reach a cell in the world while avoiding the trap.\n",
    "\n",
    "Remember that visually the world will look like this:\n",
    "\n",
    "```{figure} /lectures/mdp/dynamic-programming/grid_world.png\n",
    ":align: center\n",
    ":width: 70%\n",
    "```\n",
    "\n",
    "Now we wish to apply TD-learning to this environment and compare the results with Value Iteration.\n",
    "\n",
    "### Gym Env\n",
    "\n",
    "One advantage of TD learning is that the algorithm is model-free, so we don't need to define explicitly the transition function in the environment. \n",
    "No other modifications are required and we can use the environment for our experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GridWorld(gymnasium.Env):\n",
    "  def __init__(self):\n",
    "    # Define the action and observation spaces\n",
    "    self.action_space = spaces.Discrete(4) # Up, Down, Left, Right\n",
    "    self.observation_space = spaces.Discrete(12) # 12 cells\n",
    "    # Initialize the state\n",
    "    self.state = 0\n",
    "    self.terminals = [11, 7]\n",
    "\n",
    "  def step(self, action: int):\n",
    "\n",
    "    self._transition(action)\n",
    "    done = False\n",
    "    reward = 0\n",
    "    if self.state == 11:\n",
    "      reward = 10\n",
    "      done = True\n",
    "    elif self.state == 7:\n",
    "      reward = -10\n",
    "      done = True\n",
    "    # Return the observation, reward, done flag, and info\n",
    "    return self.state, reward, done, {}\n",
    "\n",
    "  def _transition(self, action: int):\n",
    "    \"\"\"\n",
    "    Transition function.\n",
    "    :param action: Action to take\n",
    "    \"\"\"\n",
    "    r = np.floor(self.state / 4)\n",
    "    c = self.state % 3\n",
    "\n",
    "    prob = np.random.random()\n",
    "    if prob < 0.80:\n",
    "      actual_action = action\n",
    "    elif prob < 0.90:\n",
    "      # Adjacent cell \"clockwise\"\n",
    "      actual_action = (action + 1) % 4\n",
    "    else:\n",
    "      # Adjacent cell \"counter clockwise\"\n",
    "      actual_action = (action - 1) % 4\n",
    "\n",
    "    if actual_action == 0:\n",
    "      r = max(0, r - 1)\n",
    "    elif actual_action == 2:\n",
    "      r = min(2, r + 1)\n",
    "    elif actual_action == 1:\n",
    "      c = max(0, c - 1)\n",
    "    elif actual_action == 3:\n",
    "      c = min(3, c + 1)\n",
    "    self.state = int(r * 4 + c)\n",
    "\n",
    "  def reset(self):\n",
    "    \"\"\"\n",
    "    Reset the environment.\n",
    "    \"\"\"\n",
    "    self.state = 0\n",
    "    return self.state\n",
    "\n",
    "  def render(self, render=\"human\"):\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.set_xlim(0, 4)\n",
    "    ax.set_ylim(0, 3)\n",
    "    ax.set_aspect('equal')\n",
    "\n",
    "\n",
    "    for i in range(4):\n",
    "      for j in range(3):\n",
    "        if j * 4 + i == 11:\n",
    "          rect = Rectangle((i, j), 1, 1, edgecolor='black', facecolor='green')\n",
    "          ax.add_patch(rect)\n",
    "        elif j * 4 + i == 7:\n",
    "          rect = Rectangle((i, j), 1, 1, edgecolor='black', facecolor='red')\n",
    "          ax.add_patch(rect)\n",
    "        elif j * 4 + i == 5:\n",
    "          rect = Rectangle((i, j), 1, 1, edgecolor='black', facecolor='grey')\n",
    "          ax.add_patch(rect)\n",
    "        else:\n",
    "          rect = Rectangle((i, j), 1, 1, edgecolor='black', facecolor='white')\n",
    "          ax.add_patch(rect)\n",
    "\n",
    "    ax.tick_params(axis='both',       # changes apply to both axis\n",
    "                    which='both',      # both major and minor ticks are affected\n",
    "                    bottom=False,      # ticks along the bottom edge are off\n",
    "                    top=False,         # ticks along the top edge are off\n",
    "                    left=False,\n",
    "                    right=False,\n",
    "                    labelbottom=False,\n",
    "                    labelleft=False) # labels along the bottom edge are off\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SARSA\n",
    "\n",
    "SARSA is an online reinforcement learning algorithm, so we only need to manage one policy. The implementation is trivial, but we want to make it compatible with a Gym environment. We will only consider the $\\epsilon$-greedy action selection, the other action-selections are left as a bonus exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_action_selection(q_s, epsilon):\n",
    "    rng = np.random.default_rng()\n",
    "    if rng.random() > epsilon:\n",
    "        return np.argmax(q_s)\n",
    "    else:\n",
    "        return rng.choice(len(q_s))\n",
    "\n",
    "def sarsa(env, N: int, alpha: float, epsilon: float):\n",
    "    rng = np.random.default_rng()\n",
    "\n",
    "    # We initialize the Q-values randomly.\n",
    "    # We start by generating enough random numbers for all the pair state-action.\n",
    "    # Then we reshape to obtain a table with the states as rows and actions as columns.\n",
    "    q = rng.normal(0, 1, env.observation_space.n * env.action_space.n).reshape((env.observation_space.n, env.action_space.n))\n",
    "    # The two terminal states are sets to 0 for all the actions.\n",
    "    q[env.terminals, :] = 0\n",
    "\n",
    "    for n in range(N):\n",
    "        s = env.reset()\n",
    "        done = False\n",
    "        a = greedy_action_selection(q[s,:], epsilon)\n",
    "        while not done:\n",
    "            s_next, r, done, _ = env.step(a)\n",
    "            a_next = greedy_action_selection(q[s_next,:], epsilon)\n",
    "            q[s, a] += alpha*(r + 0.9*q[s_next,a_next] - q[s, a])\n",
    "            s = s_next\n",
    "            a = a_next\n",
    "\n",
    "    # argmax gives us the highest value for each state, so the policy.\n",
    "    return np.argmax(q, axis = 1), q\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "def sarsa_conv(env, N: int, alpha: float, epsilon: float):\n",
    "    rng = np.random.default_rng()\n",
    "    q = rng.normal(0, 1, env.observation_space.n * env.action_space.n).reshape((env.observation_space.n, env.action_space.n))\n",
    "    q[env.terminals, :] = 0\n",
    "\n",
    "    q_s0 = []\n",
    "\n",
    "    for n in range(N):\n",
    "        s = env.reset()\n",
    "        done = False\n",
    "        a = greedy_action_selection(q[s,:], epsilon)\n",
    "        while not done:\n",
    "            s_next, r, done, _ = env.step(a)\n",
    "            a_next = greedy_action_selection(q[s_next,:], epsilon)\n",
    "            q[s, a] += alpha*(r + 0.9*q[s_next,a_next] - q[s, a])\n",
    "            s = s_next\n",
    "            a = a_next\n",
    "        q_s0.append(np.max(q[0, :]))\n",
    "\n",
    "    return np.argmax(q, axis = 1), q, q_s0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_sarsa_conv(q_s0):\n",
    "    plt.style.use('seaborn-v0_8')\n",
    "    x = np.linspace(0, len(q_s0), len(q_s0))\n",
    "\n",
    "    fig, ax1= plt.subplots()\n",
    "    plt.subplots_adjust(hspace=0.5)\n",
    "\n",
    "    ax1.plot(x, q_s0, linewidth=2.0, color=\"C1\")\n",
    "    ax1.set_title(\"Evolution of the value of the initial state\")\n",
    "    ax1.set_ylabel(\"Value of optimal action\")\n",
    "    ax1.set_xlabel(\"Episodes\")\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We run SARSA on our environment and save the q-values for the initial state $s_0$. Saving the value of the initial state can help us see how fast we are converging to the *optimal* policy.\n",
    "\n",
    "```{note}\n",
    "TD-learning will theoretically converge to the optimal policy for a number of episode $N$ sufficiently large.\n",
    "```\n",
    "\n",
    "Let's see what happens!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "from myst_nb import glue\n",
    "\n",
    "env = GridWorld()\n",
    "pi, q, q_s0 = sarsa_conv(env, 20, 0.1, 0.5)\n",
    "plt.style.use('seaborn-v0_8')\n",
    "x = np.linspace(0, len(q_s0), len(q_s0))\n",
    "\n",
    "fig, ax1= plt.subplots()\n",
    "plt.subplots_adjust(hspace=0.5)\n",
    "\n",
    "ax1.plot(x, q_s0, linewidth=2.0, color=\"C1\")\n",
    "ax1.set_title(\"Evolution of the value of the initial state\")\n",
    "ax1.set_ylabel(\"Value of optimal action\")\n",
    "ax1.set_xlabel(\"Episodes\")\n",
    "glue(\"sarsa_margin\", fig, display=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "````{margin} Number of episodes\n",
    "The number of episodes is crucial. Too little and the algorithm will not have time to learn a good policy. Worst it could let you think that the algorithm is not *learning*.\n",
    "\n",
    "Below is the same algorithm but only run for 20 episodes. A beginner could think that there is something wrong, but the algorithm just requires more time.\n",
    "\n",
    "```{glue:} sarsa_margin\n",
    "```\n",
    "````"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = GridWorld()\n",
    "pi, q, q_s0 = sarsa_conv(env, 1000, 0.1, 0.5)\n",
    "plot_sarsa_conv(q_s0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One can note that $V(s_0)$ fluctuated a lot during the training, it is normal and expected. These fluctuations have many explanations, the first one is the action-selection method. As we explore the different actions it explores good and bad trajectories, that have an impact of the value function. Another reason comes from the model-free approach of the algorithm. The algorithm doesn't have the transition function, so it needs to learn it during the training.\n",
    "\n",
    "#### Impact of the Hyperparameters\n",
    "\n",
    "We can change the $\\epsilon$ the exploratory ratio, and it will impact the learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = GridWorld()\n",
    "pi, q, q_s0 = sarsa_conv(env, 1000, 0.1, 0.2)\n",
    "plot_sarsa_conv(q_s0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the value function converges to a higher value by the end. It doesn't mean the policy is better! We are not evaluating the policy, we are just saving the value during training that has a bias due to the exploration. Reducing the exploration reduce the bias leading to higher values.\n",
    "\n",
    "---\n",
    "\n",
    "Now if we change the value of $\\alpha$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "from myst_nb import glue\n",
    "\n",
    "env = GridWorld()\n",
    "pi, q, q_s0 = sarsa_conv(env, 1000, 0.01, 0.5)\n",
    "plt.style.use('seaborn-v0_8')\n",
    "x = np.linspace(0, len(q_s0), len(q_s0))\n",
    "\n",
    "fig, ax1= plt.subplots()\n",
    "plt.subplots_adjust(hspace=0.5)\n",
    "\n",
    "ax1.plot(x, q_s0, linewidth=2.0, color=\"C1\")\n",
    "ax1.set_title(\"Evolution of the value of the initial state\")\n",
    "ax1.set_ylabel(\"Value of optimal action\")\n",
    "ax1.set_xlabel(\"Episodes\")\n",
    "glue(\"sarsa_alpha_margin\", fig, display=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "````{margin} Step-size\n",
    "The step-size needs to be selected carefully. A small step-size usually works better and avoid the common issue with larger ones. The worst-case scenario it will take longer to converge. \n",
    "\n",
    "Below we selected $\\alpha = 0.01$\n",
    "\n",
    "```{glue:} sarsa_alpha_margin\n",
    "```\n",
    "````"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = GridWorld()\n",
    "pi, q, q_s0 = sarsa_conv(env, 1000, 0.3, 0.5)\n",
    "plot_sarsa_conv(q_s0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Increasing $\\alpha$ increase the size of the update. On a simple problem, it helps the convergence to the optimal policy. However, it has been proven that a step size too large can lead to a longer convergence speed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q-Learning\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CSCI-531",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
