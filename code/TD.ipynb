{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "import gymnasium\n",
    "from gymnasium import spaces\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Rectangle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applying our first RL algorithms!\n",
    "\n",
    "## Grid World\n",
    "\n",
    "Let's go back to our grid world problem, where the agent start always in the same position and needs to reach a cell in the world while avoiding the trap.\n",
    "\n",
    "Remember that visually the world will look like this:\n",
    "\n",
    "```{figure} /lectures/mdp/dynamic-programming/grid_world.png\n",
    ":align: center\n",
    ":width: 70%\n",
    "```\n",
    "\n",
    "Now we wish to apply TD-learning to this environment and compare the results with Value Iteration.\n",
    "\n",
    "One advantage of TD learning is that the algorithm is model-free, so we don't need to define explicitly the transition function in the environment. \n",
    "No other modifications are required and we can use the environment for our experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GridWorld(gymnasium.Env):\n",
    "  def __init__(self):\n",
    "    # Define the action and observation spaces\n",
    "    self.action_space = spaces.Discrete(4) # Up, Down, Left, Right\n",
    "    self.observation_space = spaces.Discrete(12) # 12 cells\n",
    "    # Initialize the state\n",
    "    self.state = 0\n",
    "    self.terminals = [11, 7]\n",
    "\n",
    "  def step(self, action: int):\n",
    "\n",
    "    self._transition(action)\n",
    "    done = False\n",
    "    reward = 0\n",
    "    if self.state == 11:\n",
    "      reward = 10\n",
    "      done = True\n",
    "    elif self.state == 7:\n",
    "      reward = -10\n",
    "      done = True\n",
    "    # Return the observation, reward, done flag, and info\n",
    "    return self.state, reward, done, {}\n",
    "\n",
    "  def _transition(self, action: int):\n",
    "    \"\"\"\n",
    "    Transition function.\n",
    "    :param action: Action to take\n",
    "    \"\"\"\n",
    "    r = np.floor(self.state / 4)\n",
    "    c = self.state % 3\n",
    "\n",
    "    prob = np.random.random()\n",
    "    if prob < 0.80:\n",
    "      actual_action = action\n",
    "    elif prob < 0.90:\n",
    "      # Adjacent cell \"clockwise\"\n",
    "      actual_action = (action + 1) % 4\n",
    "    else:\n",
    "      # Adjacent cell \"counter clockwise\"\n",
    "      actual_action = (action - 1) % 4\n",
    "\n",
    "    if actual_action == 0:\n",
    "      r = max(0, r - 1)\n",
    "    elif actual_action == 2:\n",
    "      r = min(2, r + 1)\n",
    "    elif actual_action == 1:\n",
    "      c = max(0, c - 1)\n",
    "    elif actual_action == 3:\n",
    "      c = min(3, c + 1)\n",
    "    self.state = int(r * 4 + c)\n",
    "\n",
    "  def reset(self):\n",
    "    \"\"\"\n",
    "    Reset the environment.\n",
    "    \"\"\"\n",
    "    self.state = 0\n",
    "    return self.state\n",
    "\n",
    "  def render(self, render=\"human\"):\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.set_xlim(0, 4)\n",
    "    ax.set_ylim(0, 3)\n",
    "    ax.set_aspect('equal')\n",
    "\n",
    "\n",
    "    for i in range(4):\n",
    "      for j in range(3):\n",
    "        if j * 4 + i == 11:\n",
    "          rect = Rectangle((i, j), 1, 1, edgecolor='black', facecolor='green')\n",
    "          ax.add_patch(rect)\n",
    "        elif j * 4 + i == 7:\n",
    "          rect = Rectangle((i, j), 1, 1, edgecolor='black', facecolor='red')\n",
    "          ax.add_patch(rect)\n",
    "        elif j * 4 + i == 5:\n",
    "          rect = Rectangle((i, j), 1, 1, edgecolor='black', facecolor='grey')\n",
    "          ax.add_patch(rect)\n",
    "        else:\n",
    "          rect = Rectangle((i, j), 1, 1, edgecolor='black', facecolor='white')\n",
    "          ax.add_patch(rect)\n",
    "\n",
    "    ax.tick_params(axis='both',       # changes apply to both axis\n",
    "                    which='both',      # both major and minor ticks are affected\n",
    "                    bottom=False,      # ticks along the bottom edge are off\n",
    "                    top=False,         # ticks along the top edge are off\n",
    "                    left=False,\n",
    "                    right=False,\n",
    "                    labelbottom=False,\n",
    "                    labelleft=False) # labels along the bottom edge are off\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SARSA\n",
    "\n",
    "SARSA is an online reinforcement learning algorithm, so we only need to manage one policy. The implementation is trivial, but we want to make it compatible with a Gym environment. We will only consider the $\\epsilon$-greedy action selection, the other action-selections are left as a bonus exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_action_selection(q_s, epsilon):\n",
    "    rng = np.random.default_rng()\n",
    "    if rng.random() > epsilon:\n",
    "        return np.argmax(q_s)\n",
    "    else:\n",
    "        return rng.choice(len(q_s))\n",
    "\n",
    "def sarsa(env, N: int, alpha: float, epsilon: float):\n",
    "    rng = np.random.default_rng()\n",
    "\n",
    "    # We initialize the Q-values randomly.\n",
    "    # We start by generating enough random numbers for all the pair state-action.\n",
    "    # Then we reshape to obtain a table with the states as rows and actions as columns.\n",
    "    q = rng.normal(0, 1, env.observation_space.n * env.action_space.n).reshape((env.observation_space.n, env.action_space.n))\n",
    "    # The two terminal states are sets to 0 for all the actions.\n",
    "    q[env.terminals, :] = 0\n",
    "\n",
    "    for n in range(N):\n",
    "        s = env.reset()\n",
    "        done = False\n",
    "        a = greedy_action_selection(q[s,:], epsilon)\n",
    "        while not done:\n",
    "            s_next, r, done, _ = env.step(a)\n",
    "            a_next = greedy_action_selection(q[s_next,:], epsilon)\n",
    "            q[s, a] += alpha*(r + 0.9*q[s_next,a_next] - q[s, a])\n",
    "            s = s_next\n",
    "            a = a_next\n",
    "\n",
    "    # argmax gives us the highest value for each state, so the policy.\n",
    "    return np.argmax(q, axis = 1), q\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "def sarsa_conv(env, N: int, alpha: float, epsilon: float):\n",
    "    rng = np.random.default_rng()\n",
    "    q = rng.normal(0, 1, env.observation_space.n * env.action_space.n).reshape((env.observation_space.n, env.action_space.n))\n",
    "    q[env.terminals, :] = 0\n",
    "\n",
    "    q_s0 = []\n",
    "\n",
    "    for n in range(N):\n",
    "        s = env.reset()\n",
    "        done = False\n",
    "        a = greedy_action_selection(q[s,:], epsilon)\n",
    "        while not done:\n",
    "            s_next, r, done, _ = env.step(a)\n",
    "            a_next = greedy_action_selection(q[s_next,:], epsilon)\n",
    "            q[s, a] += alpha*(r + 0.9*q[s_next,a_next] - q[s, a])\n",
    "            s = s_next\n",
    "            a = a_next\n",
    "        q_s0.append(np.max(q[0, :]))\n",
    "\n",
    "    return np.argmax(q, axis = 1), q, q_s0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_conv(q_s0):\n",
    "    plt.style.use('seaborn-v0_8')\n",
    "    x = np.linspace(0, len(q_s0), len(q_s0))\n",
    "\n",
    "    fig, ax1= plt.subplots()\n",
    "    plt.subplots_adjust(hspace=0.5)\n",
    "\n",
    "    ax1.plot(x, q_s0, linewidth=2.0, color=\"C1\")\n",
    "    ax1.set_title(\"Evolution of the value of the initial state\")\n",
    "    ax1.set_ylabel(\"Value of optimal action\")\n",
    "    ax1.set_xlabel(\"Episodes\")\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We run SARSA on our environment and save the q-values for the initial state $s_0$. Saving the value of the initial state can help us see how fast we are converging to the *optimal* policy.\n",
    "\n",
    "```{note}\n",
    "TD-learning will theoretically converge to the optimal policy for a number of episode $N$ sufficiently large.\n",
    "```\n",
    "\n",
    "Let's see what happens!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "from myst_nb import glue\n",
    "\n",
    "env = GridWorld()\n",
    "pi, q, q_s0 = sarsa_conv(env, 20, 0.1, 0.5)\n",
    "plt.style.use('seaborn-v0_8')\n",
    "x = np.linspace(0, len(q_s0), len(q_s0))\n",
    "\n",
    "fig, ax1= plt.subplots()\n",
    "plt.subplots_adjust(hspace=0.5)\n",
    "\n",
    "ax1.plot(x, q_s0, linewidth=2.0, color=\"C1\")\n",
    "ax1.set_title(\"Evolution of the value of the initial state\")\n",
    "ax1.set_ylabel(\"Value of optimal action\")\n",
    "ax1.set_xlabel(\"Episodes\")\n",
    "glue(\"sarsa_margin\", fig, display=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "````{margin} Number of episodes\n",
    "The number of episodes is crucial. Too little and the algorithm will not have time to learn a good policy. Worst it could let you think that the algorithm is not *learning*.\n",
    "\n",
    "Below is the same algorithm but only run for 20 episodes. A beginner could think that there is something wrong, but the algorithm just requires more time.\n",
    "\n",
    "```{glue:} sarsa_margin\n",
    "```\n",
    "````"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = GridWorld()\n",
    "pi, q, q_s0 = sarsa_conv(env, 1000, 0.1, 0.5)\n",
    "plot_conv(q_s0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One can note that $V(s_0)$ fluctuated a lot during the training, it is normal and expected. These fluctuations have many explanations, the first one is the action-selection method. As we explore the different actions it explores good and bad trajectories, that have an impact of the value function. Another reason comes from the model-free approach of the algorithm. The algorithm doesn't have the transition function, so it needs to learn it during the training.\n",
    "\n",
    "### Impact of the Hyperparameters\n",
    "\n",
    "Most algorithm requires parameters, such as $\\epsilon$, $\\alpha$, etc. These parameters, often called **hyperarameters**, are essential to the learning process, and have a important impact on the quality of the learning. It is necessary to understand their purpose, and know how to **tune** them to obtain the desire result.\n",
    "\n",
    "```{important}\n",
    "The optimal value of each parameter depends of the problem. Having a combination that works for a problem doesn't guarantee it will work for another problem.\n",
    "Some parameters are well studied, and are known to perform (or under perform) in specific range.\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "We can change the $\\epsilon$ the exploratory ratio, and it will impact the learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = GridWorld()\n",
    "pi, q, q_s0 = sarsa_conv(env, 1000, 0.1, 0.2)\n",
    "plot_conv(q_s0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the value function converges to a higher value by the end. It doesn't mean the policy is better! We are not evaluating the policy, we are just saving the value during training that has a bias due to the exploration. Reducing the exploration reduces the bias leading to higher values.\n",
    "\n",
    "---\n",
    "\n",
    "Now if we change the value of $\\alpha$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "from myst_nb import glue\n",
    "\n",
    "env = GridWorld()\n",
    "pi, q, q_s0 = sarsa_conv(env, 1000, 0.01, 0.5)\n",
    "plt.style.use('seaborn-v0_8')\n",
    "x = np.linspace(0, len(q_s0), len(q_s0))\n",
    "\n",
    "fig, ax1= plt.subplots()\n",
    "plt.subplots_adjust(hspace=0.5)\n",
    "\n",
    "ax1.plot(x, q_s0, linewidth=2.0, color=\"C1\")\n",
    "ax1.set_title(\"Evolution of the value of the initial state\")\n",
    "ax1.set_ylabel(\"Value of optimal action\")\n",
    "ax1.set_xlabel(\"Episodes\")\n",
    "glue(\"sarsa_alpha_margin\", fig, display=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "````{margin} Step-size\n",
    "The step-size needs to be selected carefully. A small step-size usually works better and avoid the common issue with larger ones. In the worst-case scenario it will take longer to converge. \n",
    "\n",
    "Below we selected $\\alpha = 0.01$\n",
    "\n",
    "```{glue:} sarsa_alpha_margin\n",
    "```\n",
    "````"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = GridWorld()\n",
    "pi, q, q_s0 = sarsa_conv(env, 1000, 0.3, 0.5)\n",
    "plot_conv(q_s0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Increasing $\\alpha$ increase the size of the update. On a simple problem, it helps the convergence to the optimal policy. However, it has been proven that a step size too large can lead to a longer convergence speed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-Learning\n",
    "\n",
    "Q-Learning is known because it is the first off-policy reinforcement learning algorithm. It works very similarly to SARSA, but as an off-policy algorithm it has two policies, the behavior policy and the target policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_learning(env, N: int, alpha: float, epsilon: float):\n",
    "    rng = np.random.default_rng()\n",
    "\n",
    "    # We initialize the Q-values randomly.\n",
    "    # We start by generating enough random numbers for all the pair state-action.\n",
    "    # Then we reshape to obtain a table with the states as rows and actions as columns.\n",
    "    q = rng.normal(0, 1, env.observation_space.n * env.action_space.n).reshape((env.observation_space.n, env.action_space.n))\n",
    "    # The two terminal states are sets to 0 for all the actions.\n",
    "    q[env.terminals, :] = 0\n",
    "\n",
    "    for _ in range(N):\n",
    "        s = env.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            a = greedy_action_selection(q[s,:], epsilon)\n",
    "            s_next, r, done, _ = env.step(a)\n",
    "            q[s, a] += alpha*(r + 0.9*np.max(q[s_next, :]) - q[s, a])\n",
    "            s = s_next\n",
    "\n",
    "    # argmax gives us the highest value for each state, so the policy.\n",
    "    return np.argmax(q, axis = 1), q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "def q_learning_conv(env, N: int, alpha: float, epsilon: float):\n",
    "    rng = np.random.default_rng()\n",
    "\n",
    "    # We initialize the Q-values randomly.\n",
    "    # We start by generating enough random numbers for all the pair state-action.\n",
    "    # Then we reshape to obtain a table with the states as rows and actions as columns.\n",
    "    q = rng.normal(0, 1, env.observation_space.n * env.action_space.n).reshape((env.observation_space.n, env.action_space.n))\n",
    "    # The two terminal states are sets to 0 for all the actions.\n",
    "    q[env.terminals, :] = 0\n",
    "    q_s0 = []\n",
    "\n",
    "    for _ in range(N):\n",
    "        s = env.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            a = greedy_action_selection(q[s,:], epsilon)\n",
    "            s_next, r, done, _ = env.step(a)\n",
    "            q[s, a] += alpha*(r + 0.9*np.max(q[s_next, :]) - q[s, a])\n",
    "            s = s_next\n",
    "        q_s0.append(np.max(q[0, :]))\n",
    "\n",
    "    # argmax gives us the highest value for each state, so the policy.\n",
    "    return np.argmax(q, axis = 1), q, q_s0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try the algorithm and see if there is any difference with SARSA!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = GridWorld()\n",
    "pi, q, q_s0 = q_learning_conv(env, 1000, 0.1, 0.5)\n",
    "plot_conv(q_s0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q-Learning is surprisingly doing very well for an off-policy algorithm, and it converged very quickly compared to SARSA.\n",
    "\n",
    "The main reason is that the problem is simple and Q-Learning is not exploring while choosing $a'$, thus for simple problems it converges to the optimal policy very fast.\n",
    "\n",
    "### Impact of Hyperparameters\n",
    "\n",
    "We can also study the impact of these parameters on Q-learning, starting with $\\epsilon$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = GridWorld()\n",
    "pi, q, q_s0 = q_learning_conv(env, 1000, 0.1, 0.2)\n",
    "plot_conv(q_s0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reducing $\\epsilon$ reduced the exploration and led to a longer convergence period.\n",
    "\n",
    "---\n",
    "\n",
    "Let's see how $\\alpha$ impacted the learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = GridWorld()\n",
    "pi, q, q_s0 = q_learning_conv(env, 1000, 0.3, 0.5)\n",
    "plot_conv(q_s0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the same behavior that happened with SARSA. Increasing $\\alpha$ made the algorithm converged faster.\n",
    "\n",
    "## Analysis\n",
    "\n",
    "Being able to train an agent on an environment is important, but it is crucial to be able to analyze the results obtain during and after the training.\n",
    "RL algorithms are not always giving good results, and it is important to understand what we obtain from the training.\n",
    "\n",
    "### Training stability\n",
    "\n",
    "One point that is often overlooked is the stability of the training. \n",
    "\n",
    "In the previous examples, we trained our agent and we plotted the evolution of the Q-value for $s_0$. Obtaining a good result after one training doesn't guarantee that if we train again with the same parameters, we will also obtain a good policy.\n",
    "\n",
    "It two different trainings with the same parameters we could obtain very different trajectories. As the problems become larger and the algorithms more complex, the training process could be unstable. We could be lucky and obtain a good policy or unlucky and think that our methods do not work.\n",
    "\n",
    "It is possible to verify if our training is stable by training multiple time with the same parameter and see if each training looks similar. \n",
    "\n",
    "````{margin} Impact of the parameters\n",
    "\n",
    "If we change $\\epsilon$ to $0.2$, we can see that it increases the variance between each training, even if they stay relatively stable.\n",
    "\n",
    "```{glue:} TD_multiple_training_margin\n",
    "```\n",
    "````\n",
    "\n",
    "````{admonition} Example\n",
    ":class: example\n",
    "\n",
    "We run 25 training of both algorithms with the following parameter:\n",
    "- $N= 1000$\n",
    "- $\\epsilon = 0.5$\n",
    "- $\\alpha = 0.1$\n",
    "\n",
    "```{glue:} TD_multiple_training\n",
    "```\n",
    "\n",
    "In this problem with the same parameters, we can see our training is very stable.\n",
    "\n",
    "````\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "env = GridWorld()\n",
    "\n",
    "sarsa_Vs = []\n",
    "q_learning_Vs = []\n",
    "\n",
    "for _ in range(25):\n",
    "    _, _, q_s0 = sarsa_conv(env, 1000, 0.1, 0.5)\n",
    "    sarsa_Vs.append(q_s0)\n",
    "    _, _, q_s0 = q_learning_conv(env, 1000, 0.1, 0.5)\n",
    "    q_learning_Vs.append(q_s0)\n",
    "\n",
    "sarsa_Vs = np.array(sarsa_Vs)\n",
    "sarsa_Vs_mean = np.mean(sarsa_Vs, axis=0)\n",
    "sarsa_Vs_std = np.std(sarsa_Vs, axis=0)\n",
    "q_learning_Vs = np.array(q_learning_Vs)\n",
    "q_learning_mean = np.mean(q_learning_Vs, axis=0)\n",
    "q_learning_std = np.std(q_learning_Vs, axis=0)\n",
    "\n",
    "plt.style.use('seaborn-v0_8')\n",
    "x = np.linspace(0, 1000, 1000)\n",
    "\n",
    "fig, ax1= plt.subplots()\n",
    "plt.subplots_adjust(hspace=0.5)\n",
    "\n",
    "ax1.plot(x, sarsa_Vs_mean, linewidth=2.0, color=\"C1\", label=\"SARSA\")\n",
    "ax1.plot(x, q_learning_mean, linewidth=2.0, color=\"C2\", label=\"Q-Learning\")\n",
    "\n",
    "plt.fill_between(x, sarsa_Vs_mean-sarsa_Vs_std, sarsa_Vs_mean+sarsa_Vs_std,\n",
    "    alpha=0.3, facecolor='C1', antialiased=True)\n",
    "\n",
    "plt.fill_between(x, q_learning_mean-q_learning_std, q_learning_mean+q_learning_std,\n",
    "    alpha=0.3, facecolor='C2', antialiased=True)\n",
    "plt.legend(loc=\"lower right\")\n",
    "\n",
    "ax1.set_title(\"Evolution of the value of the initial state\")\n",
    "ax1.set_ylabel(\"Value of optimal action\")\n",
    "ax1.set_xlabel(\"Episodes\")\n",
    "\n",
    "\n",
    "\n",
    "glue(\"TD_multiple_training\", fig, display=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "env = GridWorld()\n",
    "\n",
    "sarsa_Vs = []\n",
    "q_learning_Vs = []\n",
    "\n",
    "for _ in range(25):\n",
    "    _, _, q_s0 = sarsa_conv(env, 1000, 0.1, 0.2)\n",
    "    sarsa_Vs.append(q_s0)\n",
    "    _, _, q_s0 = q_learning_conv(env, 1000, 0.1, 0.2)\n",
    "    q_learning_Vs.append(q_s0)\n",
    "\n",
    "sarsa_Vs = np.array(sarsa_Vs)\n",
    "sarsa_Vs_mean = np.mean(sarsa_Vs, axis=0)\n",
    "sarsa_Vs_std = np.std(sarsa_Vs, axis=0)\n",
    "q_learning_Vs = np.array(q_learning_Vs)\n",
    "q_learning_mean = np.mean(q_learning_Vs, axis=0)\n",
    "q_learning_std = np.std(q_learning_Vs, axis=0)\n",
    "\n",
    "plt.style.use('seaborn-v0_8')\n",
    "x = np.linspace(0, 1000, 1000)\n",
    "\n",
    "fig, ax1= plt.subplots()\n",
    "plt.subplots_adjust(hspace=0.5)\n",
    "\n",
    "ax1.plot(x, sarsa_Vs_mean, linewidth=2.0, color=\"C1\", label=\"SARSA\")\n",
    "ax1.plot(x, q_learning_mean, linewidth=2.0, color=\"C2\", label=\"Q-Learning\")\n",
    "\n",
    "plt.fill_between(x, sarsa_Vs_mean-sarsa_Vs_std, sarsa_Vs_mean+sarsa_Vs_std,\n",
    "    alpha=0.3, facecolor='C1', antialiased=True)\n",
    "\n",
    "plt.fill_between(x, q_learning_mean-q_learning_std, q_learning_mean+q_learning_std,\n",
    "    alpha=0.3, facecolor='C2', antialiased=True)\n",
    "plt.legend(loc=\"lower right\")\n",
    "\n",
    "ax1.set_title(\"Evolution of the value of the initial state\")\n",
    "ax1.set_ylabel(\"Value of optimal action\")\n",
    "ax1.set_xlabel(\"Episodes\")\n",
    "\n",
    "\n",
    "\n",
    "glue(\"TD_multiple_training_margin\", fig, display=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating the policy\n",
    "\n",
    "After training a policy it is necessary to evaluate the policy. Without the evaluation nothing guarantee the policy provide good results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(env, policy, N):\n",
    "    rewards = []\n",
    "    for _ in range(N):\n",
    "        s = env.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            a = policy[s]\n",
    "            s_next, r, done, _ = env.step(a)\n",
    "            s = s_next\n",
    "        rewards.append(r)\n",
    "    return rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "````{admonition} Example\n",
    ":class: example\n",
    "\n",
    "We run 1000 evaluation on both algorithms after training them with the following parameter:\n",
    "- $N= 1000$\n",
    "- $\\epsilon = 0.5$\n",
    "- $\\alpha = 0.1$\n",
    "\n",
    "```{glue:} TD_multiple_evaluation\n",
    "```\n",
    "\n",
    "We can see that the policies perform almost gives a 100% success.\n",
    "\n",
    "````"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "env = GridWorld()\n",
    "\n",
    "sarsa_p, _ = sarsa(env, 1000, 0.1, 0.5)\n",
    "sarsa_r = evaluate(env, sarsa_p, 1000)\n",
    "\n",
    "q_learning_p, _ = q_learning(env, 1000, 0.1, 0.5)\n",
    "q_learning_r = evaluate(env, q_learning_p, 1000)\n",
    "\n",
    "plt.style.use('seaborn-v0_8')\n",
    "\n",
    "fig, ax1= plt.subplots()\n",
    "plt.subplots_adjust(hspace=0.5)\n",
    "\n",
    "algs = ['SARSA', 'Q-learning']\n",
    "\n",
    "ax1.bar(algs, [np.mean(sarsa_r), np.mean(q_learning_r)], width=0.5, color=['C1', 'C2'])\n",
    "\n",
    "ax1.set_title(\"Evaluation of the policies\")\n",
    "ax1.set_ylabel(\"Average reward per episode\")\n",
    "ax1.set_xlabel(\"Algorithms\")\n",
    "\n",
    "glue(\"TD_multiple_evaluation\", fig, display=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CSCI-531",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
